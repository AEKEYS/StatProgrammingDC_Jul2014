"<!DOCTYPE html>"
"<html lang=\"en\" dir=\"ltr\" class=\"client-nojs\">"
"<head>"
"<meta charset=\"UTF-8\" />"
"<title>n-gram - Wikipedia, the free encyclopedia</title>"
"<meta name=\"generator\" content=\"MediaWiki 1.24wmf10\" />"
"<link rel=\"alternate\" href=\"android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/N-gram\" />"
"<link rel=\"alternate\" type=\"application/x-wiki\" title=\"Edit this page\" href=\"/w/index.php?title=N-gram&amp;action=edit\" />"
"<link rel=\"edit\" title=\"Edit this page\" href=\"/w/index.php?title=N-gram&amp;action=edit\" />"
"<link rel=\"apple-touch-icon\" href=\"//bits.wikimedia.org/apple-touch/wikipedia.png\" />"
"<link rel=\"shortcut icon\" href=\"//bits.wikimedia.org/favicon/wikipedia.ico\" />"
"<link rel=\"search\" type=\"application/opensearchdescription+xml\" href=\"/w/opensearch_desc.php\" title=\"Wikipedia (en)\" />"
"<link rel=\"EditURI\" type=\"application/rsd+xml\" href=\"//en.wikipedia.org/w/api.php?action=rsd\" />"
"<link rel=\"copyright\" href=\"//creativecommons.org/licenses/by-sa/3.0/\" />"
"<link rel=\"alternate\" type=\"application/atom+xml\" title=\"Wikipedia Atom feed\" href=\"/w/index.php?title=Special:RecentChanges&amp;feed=atom\" />"
"<link rel=\"canonical\" href=\"http://en.wikipedia.org/wiki/N-gram\" />"
"<link rel=\"stylesheet\" href=\"//bits.wikimedia.org/en.wikipedia.org/load.php?debug=false&amp;lang=en&amp;modules=ext.gadget.DRN-wizard%2CReferenceTooltips%2Ccharinsert%2CrefToolbar%2Cteahouse%7Cext.math.styles%7Cext.rtlcite%2Cwikihiero%7Cext.uls.nojs%7Cext.visualEditor.viewPageTarget.noscript%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.skinning.interface%7Cmediawiki.ui.button%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector&amp;*\" />"
"<meta name=\"ResourceLoaderDynamicStyles\" content=\"\" />"
"<link rel=\"stylesheet\" href=\"//bits.wikimedia.org/en.wikipedia.org/load.php?debug=false&amp;lang=en&amp;modules=site&amp;only=styles&amp;skin=vector&amp;*\" />"
"<style>a:lang(ar),a:lang(kk-arab),a:lang(mzn),a:lang(ps),a:lang(ur){text-decoration:none}"
"/* cache key: enwiki:resourceloader:filter:minify-css:7:3904d24a08aa08f6a68dc338f9be277e */</style>"
"<script src=\"//bits.wikimedia.org/en.wikipedia.org/load.php?debug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector&amp;*\"></script>"
"<script>if(window.mw){"
"mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"N-gram\",\"wgTitle\":\"N-gram\",\"wgCurRevisionId\":615028208,\"wgRevisionId\":615028208,\"wgArticleId\":986182,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Articles lacking in-text citations from February 2011\",\"All articles lacking in-text citations\",\"All articles with specifically marked weasel-worded phrases\",\"Articles with specifically marked weasel-worded phrases from November 2011\",\"Articles with specifically marked weasel-worded phrases from June 2014\",\"All articles with unsourced statements\",\"Articles with unsourced statements from November 2011\",\"Pages containing cite templates with deprecated parameters\",\"Natural language processing\",\"Computational linguistics\",\"Language modeling\",\"Speech recognition\",\"Corpus linguistics\",\"Probabilistic models\"],\"wgBreakFrames\":false,\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgMonthNamesShort\":[\"\",\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"],\"wgRelevantPageName\":\"N-gram\",\"wgIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgWikiEditorEnabledModules\":{\"toolbar\":true,\"dialogs\":true,\"hidesig\":true,\"preview\":false,\"previewDialog\":false,\"publish\":false},\"wgBetaFeaturesFeatures\":[],\"wgMediaViewerOnClick\":true,\"wgVisualEditor\":{\"isPageWatched\":false,\"magnifyClipIconURL\":\"//bits.wikimedia.org/static-1.24wmf10/skins/common/images/magnify-clip.png\",\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"svgMaxSize\":2048,\"namespacesWithSubpages\":{\"6\":0,\"8\":0,\"1\":true,\"2\":true,\"3\":true,\"4\":true,\"5\":true,\"7\":true,\"9\":true,\"10\":true,\"11\":true,\"12\":true,\"13\":true,\"14\":true,\"15\":true,\"100\":true,\"101\":true,\"102\":true,\"103\":true,\"104\":true,\"105\":true,\"106\":true,\"107\":true,\"108\":true,\"109\":true,\"110\":true,\"111\":true,\"447\":true,\"828\":true,\"829\":true}},\"wikilove-recipient\":\"\",\"wikilove-anon\":0,\"wgGuidedTourHelpGuiderUrl\":\"Help:Guided tours/guider\",\"wgFlowTermsOfUseEdit\":\"By saving changes, you agree to our \u003Ca class=\\"external text\\" href=\\"//wikimediafoundation.org/wiki/Terms_of_use\\"\u003ETerms of Use\u003C/a\u003E and agree to irrevocably release your text under the \u003Ca rel=\\"nofollow\\" class=\\"external text\\" href=\\"//creativecommons.org/licenses/by-sa/3.0\\"\u003ECC BY-SA 3.0 License\u003C/a\u003E and \u003Ca class=\\"external text\\" href=\\"//en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License\\"\u003EGFDL\u003C/a\u003E\",\"wgFlowTermsOfUseSummarize\":\"By clicking \\"Summarize\\", you agree to the terms of use for this wiki.\",\"wgFlowTermsOfUseCloseTopic\":\"By clicking \\"Close topic\\", you agree to the terms of use for this wiki.\",\"wgFlowTermsOfUseReopenTopic\":\"By clicking \\"Reopen topic\\", you agree to the terms of use for this wiki.\",\"wgULSAcceptLanguageList\":[],\"wgULSCurrentAutonym\":\"English\",\"wgFlaggedRevsParams\":{\"tags\":{\"status\":{\"levels\":1,\"quality\":2,\"pristine\":3}}},\"wgStableRevisionId\":null,\"wgCategoryTreePageCategoryOptions\":\"{\\"mode\\":0,\\"hideprefix\\":20,\\"showcount\\":true,\\"namespaces\\":false}\",\"wgNoticeProject\":\"wikipedia\",\"wgWikibaseItemId\":\"Q94489\"});"
"}</script><script>if(window.mw){"
"mw.loader.implement(\"user.options\",function($,jQuery){mw.user.options.set({\"ccmeonemails\":0,\"cols\":80,\"date\":\"default\",\"diffonly\":0,\"disablemail\":0,\"editfont\":\"default\",\"editondblclick\":0,\"editsectiononrightclick\":0,\"enotifminoredits\":0,\"enotifrevealaddr\":0,\"enotifusertalkpages\":1,\"enotifwatchlistpages\":0,\"extendwatchlist\":0,\"fancysig\":0,\"forceeditsummary\":0,\"gender\":\"unknown\",\"hideminor\":0,\"hidepatrolled\":0,\"imagesize\":2,\"math\":0,\"minordefault\":0,\"newpageshidepatrolled\":0,\"nickname\":\"\",\"norollbackdiff\":0,\"numberheadings\":0,\"previewonfirst\":0,\"previewontop\":1,\"rcdays\":7,\"rclimit\":50,\"rows\":25,\"showhiddencats\":false,\"shownumberswatching\":1,\"showtoolbar\":1,\"skin\":\"vector\",\"stubthreshold\":0,\"thumbsize\":4,\"underline\":2,\"uselivepreview\":0,\"usenewrc\":0,\"watchcreations\":1,\"watchdefault\":0,\"watchdeletion\":0,\"watchlistdays\":3,\"watchlisthideanons\":0,\"watchlisthidebots\":0,\"watchlisthideliu\":0,\"watchlisthideminor\":0,\"watchlisthideown\":0,\"watchlisthidepatrolled\":0,\"watchmoves\":0,\"wllimit\":250,"
"\"useeditwarning\":1,\"prefershttps\":1,\"flaggedrevssimpleui\":1,\"flaggedrevsstable\":0,\"flaggedrevseditdiffs\":true,\"flaggedrevsviewdiffs\":false,\"usebetatoolbar\":1,\"usebetatoolbar-cgd\":1,\"multimediaviewer-enable\":true,\"visualeditor-enable\":0,\"visualeditor-betatempdisable\":0,\"visualeditor-enable-experimental\":0,\"visualeditor-enable-language\":0,\"visualeditor-hidebetawelcome\":0,\"wikilove-enabled\":1,\"mathJax\":false,\"echo-subscriptions-web-page-review\":true,\"echo-subscriptions-email-page-review\":false,\"ep_showtoplink\":false,\"ep_bulkdelorgs\":false,\"ep_bulkdelcourses\":true,\"ep_showdyk\":true,\"echo-subscriptions-web-education-program\":true,\"echo-subscriptions-email-education-program\":false,\"echo-notify-show-link\":true,\"echo-show-alert\":true,\"echo-email-frequency\":0,\"echo-email-format\":\"html\",\"echo-subscriptions-email-system\":true,\"echo-subscriptions-web-system\":true,\"echo-subscriptions-email-user-rights\":true,\"echo-subscriptions-web-user-rights\":true,\"echo-subscriptions-email-other\":false,"
"\"echo-subscriptions-web-other\":true,\"echo-subscriptions-email-edit-user-talk\":false,\"echo-subscriptions-web-edit-user-talk\":true,\"echo-subscriptions-email-reverted\":false,\"echo-subscriptions-web-reverted\":true,\"echo-subscriptions-email-article-linked\":false,\"echo-subscriptions-web-article-linked\":false,\"echo-subscriptions-email-mention\":false,\"echo-subscriptions-web-mention\":true,\"echo-subscriptions-web-edit-thank\":true,\"echo-subscriptions-email-edit-thank\":false,\"echo-subscriptions-web-flow-discussion\":true,\"echo-subscriptions-email-flow-discussion\":false,\"gettingstarted-task-toolbar-show-intro\":true,\"uls-preferences\":\"\",\"language\":\"en\",\"variant-gan\":\"gan\",\"variant-iu\":\"iu\",\"variant-kk\":\"kk\",\"variant-ku\":\"ku\",\"variant-shi\":\"shi\",\"variant-sr\":\"sr\",\"variant-tg\":\"tg\",\"variant-uz\":\"uz\",\"variant-zh\":\"zh\",\"searchNs0\":true,\"searchNs1\":false,\"searchNs2\":false,\"searchNs3\":false,\"searchNs4\":false,\"searchNs5\":false,\"searchNs6\":false,\"searchNs7\":false,\"searchNs8\":false,\"searchNs9\":false,"
"\"searchNs10\":false,\"searchNs11\":false,\"searchNs12\":false,\"searchNs13\":false,\"searchNs14\":false,\"searchNs15\":false,\"searchNs100\":false,\"searchNs101\":false,\"searchNs108\":false,\"searchNs109\":false,\"searchNs118\":false,\"searchNs119\":false,\"searchNs446\":false,\"searchNs447\":false,\"searchNs710\":false,\"searchNs711\":false,\"searchNs828\":false,\"searchNs829\":false,\"gadget-teahouse\":1,\"gadget-ReferenceTooltips\":1,\"gadget-DRN-wizard\":1,\"gadget-charinsert\":1,\"gadget-refToolbar\":1,\"gadget-mySandbox\":1,\"variant\":\"en\"});},{},{});mw.loader.implement(\"user.tokens\",function($,jQuery){mw.user.tokens.set({\"editToken\":\"+\\\",\"patrolToken\":false,\"watchToken\":false});},{},{});"
"/* cache key: enwiki:resourceloader:filter:minify-js:7:bcb89b2f32dd43f2bd4d7fbef2aef0d5 */"
"}</script>"
"<script>if(window.mw){"
"mw.loader.load([\"mediawiki.page.startup\",\"mediawiki.legacy.wikibits\",\"mediawiki.legacy.ajax\",\"ext.centralauth.centralautologin\",\"mmv.head\",\"ext.visualEditor.viewPageTarget.init\",\"ext.uls.init\",\"ext.uls.interface\",\"ext.centralNotice.bannerController\",\"skins.vector.js\"]);"
"}</script>"
"<link rel=\"dns-prefetch\" href=\"//meta.wikimedia.org\" />"
"<!--[if lt IE 7]><style type=\"text/css\">body{behavior:url(\"/w/static-1.24wmf10/skins/vector/csshover.min.htc\")}</style><![endif]-->"
"</head>"
"<body class=\"mediawiki ltr sitedir-ltr ns-0 ns-subject page-N-gram skin-vector action-view vector-animateLayout\">"
"		<div id=\"mw-page-base\" class=\"noprint\"></div>"
"		<div id=\"mw-head-base\" class=\"noprint\"></div>"
"		<div id=\"content\" class=\"mw-body\" role=\"main\">"
"			<a id=\"top\"></a>"
"							<div id=\"siteNotice\"><!-- CentralNotice --></div>"
"						<h1 id=\"firstHeading\" class=\"firstHeading\" lang=\"en\"><span dir=\"auto\"><i>n</i>-gram</span></h1>"
"						<div id=\"bodyContent\" class=\"mw-body-content\">"
"									<div id=\"siteSub\">From Wikipedia, the free encyclopedia</div>"
"								<div id=\"contentSub\"></div>"
"												<div id=\"jump-to-nav\" class=\"mw-jump\">"
"					Jump to:					<a href=\"#mw-navigation\">navigation</a>, 					<a href=\"#p-search\">search</a>"
"				</div>"
"				<div id=\"mw-content-text\" lang=\"en\" dir=\"ltr\" class=\"mw-content-ltr\"><div class=\"hatnote\">Not to be confused with <a href=\"/wiki/Engram\" title=\"Engram\">engram</a>.</div>"
"<div class=\"hatnote\">For Google phrase-usage graphs, see <a href=\"/wiki/Google_Ngram_Viewer\" title=\"Google Ngram Viewer\">Google Ngram Viewer</a>.</div>"
"<table class=\"metadata plainlinks ambox ambox-style ambox-More_footnotes\" role=\"presentation\">"
"<tr>"
"<td class=\"mbox-image\">"
"<div style=\"width:52px;\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/40px-Text_document_with_red_question_mark.svg.png\" width=\"40\" height=\"40\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/60px-Text_document_with_red_question_mark.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/80px-Text_document_with_red_question_mark.svg.png 2x\" data-file-width=\"48\" data-file-height=\"48\" /></div>"
"</td>"
"<td class=\"mbox-text\"><span class=\"mbox-text-span\">This article includes a <a href=\"/wiki/Wikipedia:Citing_sources\" title=\"Wikipedia:Citing sources\">list of references</a>, but <b>its sources remain unclear because it has insufficient <a href=\"/wiki/Wikipedia:Citing_sources#Inline_citations\" title=\"Wikipedia:Citing sources\">inline citations</a></b>. <span class=\"hide-when-compact\">Please help to <a href=\"/wiki/Wikipedia:WikiProject_Fact_and_Reference_Check\" title=\"Wikipedia:WikiProject Fact and Reference Check\">improve</a> this article by <a href=\"/wiki/Wikipedia:When_to_cite\" title=\"Wikipedia:When to cite\">introducing</a> more precise citations.</span> <small><i>(February 2011)</i></small></span></td>"
"</tr>"
"</table>"
"<p><br />"
"In the fields of <a href=\"/wiki/Computational_linguistics\" title=\"Computational linguistics\">computational linguistics</a> and <a href=\"/wiki/Probability\" title=\"Probability\">probability</a>, an <b><i>n</i>-gram</b> is a contiguous sequence of <i>n</i> items from a given <a href=\"/wiki/Sequence\" title=\"Sequence\">sequence</a> of text or speech. The items can be <a href=\"/wiki/Phoneme\" title=\"Phoneme\">phonemes</a>, <a href=\"/wiki/Syllable\" title=\"Syllable\">syllables</a>, <a href=\"/wiki/Letter_(alphabet)\" title=\"Letter (alphabet)\">letters</a>, <a href=\"/wiki/Word\" title=\"Word\">words</a> or <a href=\"/wiki/Base_pairs\" title=\"Base pairs\" class=\"mw-redirect\">base pairs</a> according to the application. The <i>n</i>-grams typically are collected from a <a href=\"/wiki/Text_corpus\" title=\"Text corpus\">text</a> or <a href=\"/wiki/Speech_corpus\" title=\"Speech corpus\">speech corpus</a>.</p>"
"<p>An <i>n</i>-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"<a href=\"/wiki/Bigram\" title=\"Bigram\">bigram</a>\" (or, less commonly, a \"digram\"); size 3 is a \"<a href=\"/wiki/Trigram\" title=\"Trigram\">trigram</a>\". Larger sizes are sometimes referred to by the value of <i>n</i>, e.g., \"four-gram\", \"five-gram\", and so on.</p>"
"<p></p>"
"<div id=\"toc\" class=\"toc\">"
"<div id=\"toctitle\">"
"<h2>Contents</h2>"
"</div>"
"<ul>"
"<li class=\"toclevel-1 tocsection-1\"><a href=\"#Applications\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">Applications</span></a></li>"
"<li class=\"toclevel-1 tocsection-2\"><a href=\"#Examples\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Examples</span></a></li>"
"<li class=\"toclevel-1 tocsection-3\"><a href=\"#n-gram_models\"><span class=\"tocnumber\">3</span> <span class=\"toctext\"><i>n</i>-gram models</span></a></li>"
"<li class=\"toclevel-1 tocsection-4\"><a href=\"#Applications_and_considerations\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">Applications and considerations</span></a></li>"
"<li class=\"toclevel-1 tocsection-5\"><a href=\"#n-grams_for_approximate_matching\"><span class=\"tocnumber\">5</span> <span class=\"toctext\"><i>n</i>-grams for approximate matching</span></a></li>"
"<li class=\"toclevel-1 tocsection-6\"><a href=\"#Other_applications\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">Other applications</span></a></li>"
"<li class=\"toclevel-1 tocsection-7\"><a href=\"#Bias-versus-variance_trade-off\"><span class=\"tocnumber\">7</span> <span class=\"toctext\">Bias-versus-variance trade-off</span></a>"
"<ul>"
"<li class=\"toclevel-2 tocsection-8\"><a href=\"#Smoothing_techniques\"><span class=\"tocnumber\">7.1</span> <span class=\"toctext\">Smoothing techniques</span></a></li>"
"<li class=\"toclevel-2 tocsection-9\"><a href=\"#Skip-Gram\"><span class=\"tocnumber\">7.2</span> <span class=\"toctext\">Skip-Gram</span></a></li>"
"</ul>"
"</li>"
"<li class=\"toclevel-1 tocsection-10\"><a href=\"#See_also\"><span class=\"tocnumber\">8</span> <span class=\"toctext\">See also</span></a></li>"
"<li class=\"toclevel-1 tocsection-11\"><a href=\"#References\"><span class=\"tocnumber\">9</span> <span class=\"toctext\">References</span></a></li>"
"<li class=\"toclevel-1 tocsection-12\"><a href=\"#External_links\"><span class=\"tocnumber\">10</span> <span class=\"toctext\">External links</span></a></li>"
"</ul>"
"</div>"
"<p></p>"
"<h2><span class=\"mw-headline\" id=\"Applications\">Applications</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=1\" title=\"Edit section: Applications\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>"
"<p>An <b><i>n</i>-gram model</b> is a type of probabilistic <a href=\"/wiki/Language_model\" title=\"Language model\">language model</a> for predicting the next item in such a sequence in the form of a <img class=\"mwe-math-fallback-png-inline tex\" alt=\"(n - 1)\" src=\"//upload.wikimedia.org/math/c/e/e/cee44a4736519848cd908612350c85fe.png\" />–order <a href=\"/wiki/Markov_chain\" title=\"Markov chain\">Markov model</a>.<sup id=\"cite_ref-1\" class=\"reference\"><a href=\"#cite_note-1\"><span>[</span>1<span>]</span></a></sup> <i>n</i>-gram models are now widely used in <a href=\"/wiki/Probability\" title=\"Probability\">probability</a>, <a href=\"/wiki/Communication_theory\" title=\"Communication theory\">communication theory</a>, <a href=\"/wiki/Computational_linguistics\" title=\"Computational linguistics\">computational linguistics</a> (for instance, statistical <a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">natural language processing</a>), <a href=\"/wiki/Computational_biology\" title=\"Computational biology\">computational biology</a> (for instance, biological <a href=\"/wiki/Sequence_analysis\" title=\"Sequence analysis\">sequence analysis</a>), and <a href=\"/wiki/Data_compression\" title=\"Data compression\">data compression</a>. The two core advantages<sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">[<i><a href=\"/wiki/Wikipedia:Avoid_weasel_words\" title=\"Wikipedia:Avoid weasel words\" class=\"mw-redirect\"><span>compared to?</span></a></i>]</sup> of <i>n</i>-gram models (and algorithms that use them) are relative simplicity and the ability to scale up – by simply increasing <i>n</i> a model can be used to store more context with a well-understood <a href=\"/wiki/Space%E2%80%93time_tradeoff\" title=\"Space–time tradeoff\">space–time tradeoff</a>, enabling small experiments to scale up very efficiently.</p>"
"<h2><span class=\"mw-headline\" id=\"Examples\">Examples</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=2\" title=\"Edit section: Examples\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>"
"<table class=\"wikitable\" style=\"font-size:85%;\">"
"<caption>Figure 1 <i>n</i>-gram examples from various disciplines</caption>"
"<tr>"
"<th>Field</th>"
"<th>Unit</th>"
"<th>Sample sequence</th>"
"<th>1-gram sequence</th>"
"<th>2-gram sequence</th>"
"<th>3-gram sequence</th>"
"</tr>"
"<tr>"
"<th>Vernacular name</th>"
"<th></th>"
"<th></th>"
"<th>unigram</th>"
"<th>bigram</th>"
"<th>trigram</th>"
"</tr>"
"<tr>"
"<th>Order of resulting <a href=\"/wiki/Markov_model\" title=\"Markov model\">Markov model</a></th>"
"<th></th>"
"<th></th>"
"<th>0</th>"
"<th>1</th>"
"<th>2</th>"
"</tr>"
"<tr>"
"<td><a href=\"/wiki/Protein_sequencing\" title=\"Protein sequencing\">Protein sequencing</a></td>"
"<td><a href=\"/wiki/Amino_acid\" title=\"Amino acid\">amino acid</a></td>"
"<td>… Cys-Gly-Leu-Ser-Trp …</td>"
"<td>…, Cys, Gly, Leu, Ser, Trp, …</td>"
"<td>…, Cys-Gly, Gly-Leu, Leu-Ser, Ser-Trp, …</td>"
"<td>…, Cys-Gly-Leu, Gly-Leu-Ser, Leu-Ser-Trp, …</td>"
"</tr>"
"<tr>"
"<td><a href=\"/wiki/DNA_sequencing\" title=\"DNA sequencing\">DNA sequencing</a></td>"
"<td><a href=\"/wiki/Base_pair\" title=\"Base pair\">base pair</a></td>"
"<td>…AGCTTCGA…</td>"
"<td>…, A, G, C, T, T, C, G, A, …</td>"
"<td>…, AG, GC, CT, TT, TC, CG, GA, …</td>"
"<td>…, AGC, GCT, CTT, TTC, TCG, CGA, …</td>"
"</tr>"
"<tr>"
"<td><a href=\"/wiki/Computational_linguistics\" title=\"Computational linguistics\">Computational linguistics</a></td>"
"<td><a href=\"/wiki/Character_(computing)\" title=\"Character (computing)\">character</a></td>"
"<td>…to_be_or_not_to_be…</td>"
"<td>…, t, o, _, b, e, _, o, r, _, n, o, t, _, t, o, _, b, e, …</td>"
"<td>…, to, o_, _b, be, e_, _o, or, r_, _n, no, ot, t_, _t, to, o_, _b, be, …</td>"
"<td>…, to_, o_b, _be, be_, e_o, _or, or_, r_n, _no, not, ot_, t_t, _to, to_, o_b, _be, …</td>"
"</tr>"
"<tr>"
"<td><a href=\"/wiki/Computational_linguistics\" title=\"Computational linguistics\">Computational linguistics</a></td>"
"<td><a href=\"/wiki/Word\" title=\"Word\">word</a></td>"
"<td>… to be or not to be …</td>"
"<td>…, to, be, or, not, to, be, …</td>"
"<td>…, to be, be or, or not, not to, to be, …</td>"
"<td>…, to be or, be or not, or not to, not to be, …</td>"
"</tr>"
"</table>"
"<p>Figure 1 shows several example sequences and the corresponding 1-gram, 2-gram and 3-gram sequences.</p>"
"<p>Here are further examples; these are word-level 3-grams and 4-grams (and counts of the number of times they appeared) from the Google <i>n</i>-gram corpus.<sup id=\"cite_ref-2\" class=\"reference\"><a href=\"#cite_note-2\"><span>[</span>2<span>]</span></a></sup></p>"
"<ul>"
"<li>ceramics collectables collectibles (55)</li>"
"<li>ceramics collectables fine (130)</li>"
"<li>ceramics collected by (52)</li>"
"<li>ceramics collectible pottery (50)</li>"
"<li>ceramics collectibles cooking (45)</li>"
"</ul>"
"<p>4-grams</p>"
"<ul>"
"<li>serve as the incoming (92)</li>"
"<li>serve as the incubator (99)</li>"
"<li>serve as the independent (794)</li>"
"<li>serve as the index (223)</li>"
"<li>serve as the indication (72)</li>"
"<li>serve as the indicator (120)</li>"
"</ul>"
"<h2><span class=\"mw-headline\" id=\"n-gram_models\"><i>n</i>-gram models</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=3\" title=\"Edit section: n-gram models\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>"
"<p>An <b><i>n</i>-gram model</b> models sequences, notably natural languages, using the statistical properties of <i>n</i>-grams.</p>"
"<p>This idea can be traced to an experiment by <a href=\"/wiki/Claude_Shannon\" title=\"Claude Shannon\">Claude Shannon</a>'s work in <a href=\"/wiki/Information_theory\" title=\"Information theory\">information theory</a>. Shannon posed the question: given a sequence of letters (for example, the sequence \"for ex\"), what is the <a href=\"/wiki/Likelihood\" title=\"Likelihood\">likelihood</a> of the next letter? From training data, one can derive a <a href=\"/wiki/Probability_distribution\" title=\"Probability distribution\">probability distribution</a> for the next letter given a history of size <img class=\"mwe-math-fallback-png-inline tex\" alt=\"n\" src=\"//upload.wikimedia.org/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png\" />: <i>a</i> = 0.4, <i>b</i> = 0.00001, <i>c</i> = 0, ....; where the probabilities of all possible \"next-letters\" sum to 1.0...</p>"
"<p>More concisely, an <i>n</i>-gram model predicts <img class=\"mwe-math-fallback-png-inline tex\" alt=\"x_{i}\" src=\"//upload.wikimedia.org/math/6/4/b/64b94619c0031522d16b05a37bc06189.png\" /> based on <img class=\"mwe-math-fallback-png-inline tex\" alt=\"x_{i-(n-1)}, \dots, x_{i-1}\" src=\"//upload.wikimedia.org/math/8/2/9/829ed6a6b87de8424ccb6b1807226718.png\" />. In probability terms, this is <img class=\"mwe-math-fallback-png-inline tex\" alt=\"P(x_{i} | x_{i-(n-1)}, \dots, x_{i-1})\" src=\"//upload.wikimedia.org/math/8/6/a/86a4cac2c74bfd076b92605fcc429f2d.png\" />. When used for <a href=\"/wiki/Language_model\" title=\"Language model\">language modeling</a>, independence assumptions are made so that each word depends only on the last <i>n-1</i> words. This <a href=\"/wiki/Markov_model\" title=\"Markov model\">Markov model</a> is used as an approximation of the true underlying language. This assumption is important because it massively simplifies the problem of learning the language model from data. In addition, because of the open nature of language, it is common to group words unknown to the language model together.</p>"
"<p>Note that in a simple n-gram language model, the probability of a word, conditioned on some number of previous words (one word in a bigram model, two words in a trigram model, etc.) can be described as following a <a href=\"/wiki/Categorical_distribution\" title=\"Categorical distribution\">categorical distribution</a> (often imprecisely called a \"<a href=\"/wiki/Multinomial_distribution\" title=\"Multinomial distribution\">multinomial distribution</a>\").</p>"
"<p>In practice, the probability distributions are smoothed by assigning non-zero probabilities to unseen words or n-grams; see <a href=\"/wiki/N-gram#Smoothing_techniques\" title=\"N-gram\">smoothing techniques</a>.</p>"
"<h2><span class=\"mw-headline\" id=\"Applications_and_considerations\">Applications and considerations</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=4\" title=\"Edit section: Applications and considerations\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>"
"<p><i>n</i>-gram models are widely used in statistical <a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">natural language processing</a>. In <a href=\"/wiki/Speech_recognition\" title=\"Speech recognition\">speech recognition</a>, <a href=\"/wiki/Phonemes\" title=\"Phonemes\" class=\"mw-redirect\">phonemes</a> and sequences of phonemes are modeled using a <i>n</i>-gram distribution. For parsing, words are modeled such that each <i>n</i>-gram is composed of <i>n</i> words. For <a href=\"/wiki/Language_identification\" title=\"Language identification\">language identification</a>, sequences of <a href=\"/wiki/Character_(symbol)\" title=\"Character (symbol)\">characters</a>/<a href=\"/wiki/Grapheme\" title=\"Grapheme\">graphemes</a> (<i>e.g.</i>, <a href=\"/wiki/Letter_(alphabet)\" title=\"Letter (alphabet)\">letters of the alphabet</a>) are modeled for different languages.<sup id=\"cite_ref-3\" class=\"reference\"><a href=\"#cite_note-3\"><span>[</span>3<span>]</span></a></sup> For sequences of characters, the 3-grams (sometimes referred to as \"trigrams\") that can be generated from \"good morning\" are \"goo\", \"ood\", \"od \", \"d m\", \" mo\", \"mor\" and so forth (sometimes the beginning and end of a text are modeled explicitly, adding \"__g\", \"_go\", \"ng_\", and \"g__\"). For sequences of words, the trigrams that can be generated from \"the dog smelled like a skunk\" are \"# the dog\", \"the dog smelled\", \"dog smelled like\", \"smelled like a\", \"like a skunk\" and \"a skunk #\". Some<sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">[<i><a href=\"/wiki/Wikipedia:Avoid_weasel_words\" title=\"Wikipedia:Avoid weasel words\" class=\"mw-redirect\"><span title=\"The material near this tag possibly uses too-vague attribution or weasel words. (June 2014)\">who?</span></a></i>]</sup> practitioners preprocess strings to remove spaces, most<sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">[<i><a href=\"/wiki/Wikipedia:Avoid_weasel_words\" title=\"Wikipedia:Avoid weasel words\" class=\"mw-redirect\"><span title=\"The material near this tag possibly uses too-vague attribution or weasel words. (June 2014)\">who?</span></a></i>]</sup> simply collapse <a href=\"/wiki/Whitespace_character\" title=\"Whitespace character\">whitespace</a> to a single space while preserving paragraph marks. Punctuation is also commonly reduced or removed by preprocessing. <i>n</i>-grams can also be used for sequences of words or almost any type of data. For example, they have been used for extracting features for clustering large sets of satellite earth images and for determining what part of the Earth a particular image came from.<sup id=\"cite_ref-4\" class=\"reference\"><a href=\"#cite_note-4\"><span>[</span>4<span>]</span></a></sup> They have also been very successful as the first pass in genetic sequence search and in the identification of the species from which short sequences of DNA originated.<sup id=\"cite_ref-5\" class=\"reference\"><a href=\"#cite_note-5\"><span>[</span>5<span>]</span></a></sup></p>"
"<p><i>n</i>-gram models are often criticized because they lack any explicit representation of long range dependency. (In fact, it was <a href=\"/wiki/Noam_Chomsky\" title=\"Noam Chomsky\">Chomsky</a>'s critique of <a href=\"/wiki/Markov_model\" title=\"Markov model\">Markov models</a> in the late 1950s that caused their virtual disappearance from <a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">natural language processing</a>, along with statistical methods in general, until well into the 1980s.) This is because the only explicit dependency range is (<i>n</i>-1) tokens for an <i>n</i>-gram model, and since natural languages incorporate many cases of unbounded dependencies (such as <a href=\"/wiki/Wh-movement\" title=\"Wh-movement\">wh-movement</a>), this means that an <i>n</i>-gram model cannot in principle distinguish unbounded dependencies from noise (since long range correlations drop exponentially with distance for any Markov model). For this reason, <i>n</i>-gram models have not made much impact on linguistic theory, where part of the explicit goal is to model such dependencies.</p>"
"<p>Another criticism that has been made is that Markov models of language, including n-gram models, do not explicitly capture the performance/competence distinction discussed by Chomsky. This is because n-gram models are not designed to model linguistic knowledge as such, and make no claims to being (even potentially) complete models of linguistic knowledge; instead, they are used in practical applications.</p>"
"<p>In practice, n-gram models have been shown to be extremely effective in modeling language data, which is a core component in modern statistical <a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">language</a> applications. Most modern applications that rely on n-gram based models, such as <a href=\"/wiki/Machine_translation\" title=\"Machine translation\">machine translation</a> applications, do not rely exclusively on such models; instead, they typically also incorporate <a href=\"/wiki/Bayesian_inference\" title=\"Bayesian inference\">Bayesian inference</a>. Modern statistical models are typically made up of two parts, a <a href=\"/wiki/Prior_distribution\" title=\"Prior distribution\" class=\"mw-redirect\">prior distribution</a> describing the inherent likelihood of a possible result and a <a href=\"/wiki/Likelihood_function\" title=\"Likelihood function\">likelihood function</a> used to assess the compatibility of a possible result with observed data. When a language model is used, it is used as part of the prior distribution (e.g. to gauge the inherent \"goodness\" of a possible translation), and even then it is often not the only component in this distribution. Handcrafted features of various sorts are also used, for example variables that represent the position of a word in a sentence or the general topic of discourse. In addition, features based on the structure of the potential result, such as syntactic considerations, are often used. Such features are also used as part of the likelihood function, which makes use of the observed data. Conventional linguistic theory can be incorporated in these features (although in practice, it is rare that features specific to generative or other particular theories of grammar are incorporated, as <a href=\"/wiki/Computational_linguistics\" title=\"Computational linguistics\">computational linguists</a> tend to be \"agnostic\" towards individual theories of grammar<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">[<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (November 2011)\">citation needed</span></a></i>]</sup>).</p>"
"<h2><span class=\"mw-headline\" id=\"n-grams_for_approximate_matching\"><i>n</i>-grams for approximate matching</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=5\" title=\"Edit section: n-grams for approximate matching\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>"
"<div class=\"hatnote relarticle mainarticle\">Main article: <a href=\"/wiki/Approximate_string_matching\" title=\"Approximate string matching\">Approximate string matching</a></div>"
"<p><i>n</i>-grams can also be used for efficient approximate matching. By converting a sequence of items to a set of <i>n</i>-grams, it can be embedded in a <a href=\"/wiki/Vector_space\" title=\"Vector space\">vector space</a>, thus allowing the sequence to be compared to other sequences in an efficient manner. For example, if we convert strings with only letters in the English alphabet into 3-grams, we get a <img class=\"mwe-math-fallback-png-inline tex\" alt=\"26^3\" src=\"//upload.wikimedia.org/math/2/6/e/26edf679bf9aee54cacde8ca9b791444.png\" />-dimensional space (the first dimension measures the number of occurrences of \"aaa\", the second \"aab\", and so forth for all possible combinations of three letters). Using this representation, we lose information about the string. For example, both the strings \"abc\" and \"bca\" give rise to exactly the same 2-gram \"bc\" (although {\"ab\", \"bc\"} is clearly not the same as {\"bc\", \"ca\"}). However, we know empirically that if two strings of real text have a similar vector representation (as measured by <a href=\"/wiki/Cosine_similarity\" title=\"Cosine similarity\">cosine distance</a>) then they are likely to be similar. Other metrics have also been applied to vectors of <i>n</i>-grams with varying, sometimes better, results. For example <a href=\"/wiki/Z-score\" title=\"Z-score\" class=\"mw-redirect\">z-scores</a> have been used to compare documents by examining how many standard deviations each <i>n</i>-gram differs from its mean occurrence in a large collection, or <a href=\"/wiki/Text_corpus\" title=\"Text corpus\">text corpus</a>, of documents (which form the \"background\" vector). In the event of small counts, the <a href=\"/w/index.php?title=G-score&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"G-score (page does not exist)\">g-score</a> may give better results for comparing alternative models.</p>"
"<p>It is also possible to take a more principled approach to the statistics of <i>n</i>-grams, modeling similarity as the likelihood that two strings came from the same source directly in terms of a problem in <a href=\"/wiki/Bayesian_inference\" title=\"Bayesian inference\">Bayesian inference</a>.</p>"
"<p><i>n</i>-gram-based searching can also be used for <a href=\"/wiki/Plagiarism_detection\" title=\"Plagiarism detection\">plagiarism detection</a>.</p>"
"<h2><span class=\"mw-headline\" id=\"Other_applications\">Other applications</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=6\" title=\"Edit section: Other applications\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>"
"<p><i>n</i>-grams find use in several areas of computer science, <a href=\"/wiki/Computational_linguistics\" title=\"Computational linguistics\">computational linguistics</a>, and applied mathematics.</p>"
"<p>They have been used to:</p>"
"<ul>"
"<li>design <a href=\"/wiki/Kernel_trick\" title=\"Kernel trick\" class=\"mw-redirect\">kernels</a> that allow <a href=\"/wiki/Machine_learning\" title=\"Machine learning\">machine learning</a> algorithms such as <a href=\"/wiki/Support_vector_machine\" title=\"Support vector machine\">support vector machines</a> to learn from string data</li>"
"<li>find likely candidates for the correct spelling of a misspelled word</li>"
"<li>improve compression in <a href=\"/wiki/Data_compression\" title=\"Data compression\">compression algorithms</a> where a small area of data requires <i>n</i>-grams of greater length</li>"
"<li>assess the probability of a given word sequence appearing in text of a language of interest in pattern recognition systems, <a href=\"/wiki/Speech_recognition\" title=\"Speech recognition\">speech recognition</a>, OCR (<a href=\"/wiki/Optical_character_recognition\" title=\"Optical character recognition\">optical character recognition</a>), <a href=\"/wiki/Intelligent_Character_Recognition\" title=\"Intelligent Character Recognition\" class=\"mw-redirect\">Intelligent Character Recognition</a> (<a href=\"/wiki/Intelligent_character_recognition\" title=\"Intelligent character recognition\">ICR</a>), <a href=\"/wiki/Machine_translation\" title=\"Machine translation\">machine translation</a> and similar applications</li>"
"<li>improve retrieval in <a href=\"/wiki/Information_retrieval\" title=\"Information retrieval\">information retrieval</a> systems when it is hoped to find similar \"documents\" (a term for which the conventional meaning is sometimes stretched, depending on the data set) given a single query document and a database of reference documents</li>"
"<li>improve retrieval performance in genetic sequence analysis as in the <a href=\"/wiki/BLAST\" title=\"BLAST\">BLAST</a> family of programs</li>"
"<li>identify the language a text is in or the species a small sequence of DNA was taken from</li>"
"<li>predict letters or words at random in order to create text, as in the <a href=\"/wiki/Dissociated_press\" title=\"Dissociated press\">dissociated press</a> algorithm.</li>"
"</ul>"
"<h2><span class=\"mw-headline\" id=\"Bias-versus-variance_trade-off\">Bias-versus-variance trade-off</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=7\" title=\"Edit section: Bias-versus-variance trade-off\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>"
"<p>What goes into picking the <i>n</i> for the <i>n</i>-gram?</p>"
"<p>With <i>n</i>-gram models it is necessary to find the right trade off between the stability of the estimate against its appropriateness. This means that trigram (i.e. triplets of words) is a common choice with large training corpora (millions of words), whereas a bigram is often used with smaller ones.</p>"
"<h3><span class=\"mw-headline\" id=\"Smoothing_techniques\">Smoothing techniques</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=8\" title=\"Edit section: Smoothing techniques\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>"
"<p>There are problems of balance weight between <i>infrequent grams</i> (for example, if a proper name appeared in the training data) and <i>frequent grams</i>. Also, items not seen in the training data will be given a <a href=\"/wiki/Probability\" title=\"Probability\">probability</a> of 0.0 without <a href=\"/wiki/Smoothing\" title=\"Smoothing\">smoothing</a>. For unseen but plausible data from a sample, one can introduce <a href=\"/wiki/Pseudocount\" title=\"Pseudocount\">pseudocounts</a>. Pseudocounts are generally motivated on Bayesian grounds.</p>"
"<p>In practice it is necessary to <i>smooth</i> the probability distributions by also assigning non-zero probabilities to unseen words or n-grams. The reason is that models derived directly from the n-gram frequency counts have severe problems when confronted with any n-grams that have not explicitly been seen before -- <a href=\"/wiki/PPM_compression_algorithm\" title=\"PPM compression algorithm\" class=\"mw-redirect\">the zero-frequency problem</a>. Various smoothing methods are used, from simple \"add-one\" (Laplace) smoothing (assign a count of 1 to unseen n-grams; see <a href=\"/wiki/Rule_of_succession\" title=\"Rule of succession\">Rule of succession</a>) to more sophisticated models, such as <a href=\"/wiki/Good-Turing_discounting\" title=\"Good-Turing discounting\" class=\"mw-redirect\">Good-Turing discounting</a> or <a href=\"/wiki/Katz%27s_back-off_model\" title=\"Katz's back-off model\">back-off models</a>. Some of these methods are equivalent to assigning a <a href=\"/wiki/Prior_distribution\" title=\"Prior distribution\" class=\"mw-redirect\">prior distribution</a> to the probabilities of the N-grams and using <a href=\"/wiki/Bayesian_inference\" title=\"Bayesian inference\">Bayesian inference</a> to compute the resulting <a href=\"/wiki/Posterior_distribution\" title=\"Posterior distribution\" class=\"mw-redirect\">posterior</a> N-gram probabilities. However, the more sophisticated smoothing models were typically not derived in this fashion, but instead through independent considerations.</p>"
"<h3><span class=\"mw-headline\" id=\"Skip-Gram\">Skip-Gram</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=9\" title=\"Edit section: Skip-Gram\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>"
"<p>In the field of <a href=\"/wiki/Computational_linguistics\" title=\"Computational linguistics\">computational linguistics</a>, in particular <a href=\"/wiki/Language_model\" title=\"Language model\">language modeling</a>, <b>skip-grams</b><sup id=\"cite_ref-6\" class=\"reference\"><a href=\"#cite_note-6\"><span>[</span>6<span>]</span></a></sup> are a generalization of n-grams in which the components (typically words) need not be consecutive in the text under consideration, but may leave gaps that are <i>skipped</i> over.<sup id=\"cite_ref-7\" class=\"reference\"><a href=\"#cite_note-7\"><span>[</span>7<span>]</span></a></sup> They provide one way of overcoming the <a href=\"/w/index.php?title=Data_sparsity_problem&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Data sparsity problem (page does not exist)\">data sparsity problem</a> found with conventional n-gram analysis.</p>"
"<p>Formally, an <span class=\"texhtml mvar\" style=\"font-style:italic;\">n</span>-gram is a consecutive subsequence of length <span class=\"texhtml mvar\" style=\"font-style:italic;\">n</span> of some sequence of tokens <span class=\"texhtml\"><i>w</i>₁ … <i>wₙ</i></span>. A <span class=\"texhtml mvar\" style=\"font-style:italic;\">k</span>-skip-<span class=\"texhtml mvar\" style=\"font-style:italic;\">n</span>-gram is a length-<span class=\"texhtml mvar\" style=\"font-style:italic;\">n</span> subsequence where the components occur at distance at most <span class=\"texhtml mvar\" style=\"font-style:italic;\">k</span> from each other.</p>"
"<p>For example, in the input text:</p>"
"<dl>"
"<dd><i>the rain in Spain falls mainly on the plain</i></dd>"
"</dl>"
"<p>the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences</p>"
"<dl>"
"<dd><i>the in</i>, <i>rain Spain</i>, <i>in falls</i>, <i>Spain mainly</i>, <i>mainly the</i> and <i>on plain</i>.</dd>"
"</dl>"
"<ul>"
"<li><a href=\"/wiki/Linear_interpolation\" title=\"Linear interpolation\">Linear interpolation</a> (e.g., taking the <a href=\"/wiki/Weighted_mean\" title=\"Weighted mean\" class=\"mw-redirect\">weighted mean</a> of the unigram, bigram, and trigram)</li>"
"<li><a href=\"/wiki/Good-Turing\" title=\"Good-Turing\" class=\"mw-redirect\">Good-Turing</a> discounting</li>"
"<li><a href=\"/w/index.php?title=Witten-Bell_discounting&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Witten-Bell discounting (page does not exist)\">Witten-Bell discounting</a></li>"
"<li><a href=\"/wiki/Additive_smoothing\" title=\"Additive smoothing\">Lidstone's smoothing</a></li>"
"<li><a href=\"/wiki/Katz%27s_back-off_model\" title=\"Katz's back-off model\">Katz's back-off model</a> (trigram)</li>"
"<li><a href=\"/w/index.php?title=Kneser-Ney_smoothing&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Kneser-Ney smoothing (page does not exist)\">Kneser-Ney smoothing</a></li>"
"</ul>"
"<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=10\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>"
"<ul>"
"<li><a href=\"/wiki/Collocation\" title=\"Collocation\">Collocation</a></li>"
"<li><a href=\"/wiki/Hidden_Markov_model\" title=\"Hidden Markov model\">Hidden Markov model</a></li>"
"<li><a href=\"/wiki/N-tuple\" title=\"N-tuple\" class=\"mw-redirect\">n-tuple</a></li>"
"<li><a href=\"/wiki/K-mer\" title=\"K-mer\">k-mer</a></li>"
"<li><a href=\"/wiki/String_Kernel\" title=\"String Kernel\" class=\"mw-redirect\">String Kernel</a></li>"
"<li><a href=\"/wiki/MinHash\" title=\"MinHash\">MinHash</a></li>"
"<li><a href=\"/wiki/Feature_extraction\" title=\"Feature extraction\">Feature extraction</a></li>"
"</ul>"
"<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=11\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>"
"<div class=\"reflist\" style=\"list-style-type: decimal;\">"
"<ol class=\"references\">"
"<li id=\"cite_note-1\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-1\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external free\" href=\"https://class.coursera.org/nlp/lecture/17\">https://class.coursera.org/nlp/lecture/17</a></span></li>"
"<li id=\"cite_note-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-2\">^</a></b></span> <span class=\"reference-text\"><span class=\"citation web\">Alex Franz and Thorsten Brants (2006). <a rel=\"nofollow\" class=\"external text\" href=\"http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html\">\"All Our N-gram are Belong to You\"</a>. <i>Google Research Blog</i><span class=\"reference-accessdate\">. Retrieved 2011-12-16</span>.</span><span title=\"ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram&amp;rft.atitle=All+Our+N-gram+are+Belong+to+You&amp;rft.au=Alex+Franz+and+Thorsten+Brants&amp;rft.aulast=Alex+Franz+and+Thorsten+Brants&amp;rft.date=2006&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fgoogleresearch.blogspot.com%2F2006%2F08%2Fall-our-n-gram-are-belong-to-you.html&amp;rft.jtitle=Google+Research+Blog&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span></li>"
"<li id=\"cite_note-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-3\">^</a></b></span> <span class=\"reference-text\"><span class=\"citation journal\">Ted Dunning (1994). <a rel=\"nofollow\" class=\"external text\" href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.1958\"><i>Statistical Identification of Language</i></a>. New Mexico State University.</span><span title=\"ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram&amp;rft.aulast=Ted+Dunning&amp;rft.au=Ted+Dunning&amp;rft.btitle=Statistical+Identification+of+Language&amp;rft.date=1994&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.48.1958&amp;rft.pub=New+Mexico+State+University&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span> Technical Report MCCS 94-273</span></li>"
"<li id=\"cite_note-4\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-4\">^</a></b></span> <span class=\"reference-text\">Soffer, A., \"Image categorization using texture features,\" Document Analysis and Recognition, 1997., Proceedings of the Fourth International Conference on , vol.1, no., pp.233,237 vol.1, 18-20 Aug 1997 doi: 10.1109/ICDAR.1997.619847</span></li>"
"<li id=\"cite_note-5\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-5\">^</a></b></span> <span class=\"reference-text\">Andrija Tomović, Predrag Janičić, Vlado Kešelj, n-Gram-based classification and unsupervised hierarchical clustering of genome sequences, Computer Methods and Programs in Biomedicine, Volume 81, Issue 2, February 2006, Pages 137-153, ISSN 0169-2607, <a rel=\"nofollow\" class=\"external free\" href=\"http://dx.doi.org/10.1016/j.cmpb.2005.11.007\">http://dx.doi.org/10.1016/j.cmpb.2005.11.007</a>.</span></li>"
"<li id=\"cite_note-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-6\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external free\" href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.1629\">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.1629</a></span></li>"
"<li id=\"cite_note-7\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-7\">^</a></b></span> <span class=\"reference-text\"><span class=\"citation web\">David Guthrie et al. (2006). <a rel=\"nofollow\" class=\"external text\" href=\"http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf\">\"A Closer Look at Skip-gram Modelling\"</a>.</span><span title=\"ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram&amp;rft.au=David+Guthrie+et+al.&amp;rft.aulast=David+Guthrie+et+al.&amp;rft.btitle=A+Closer+Look+at+Skip-gram+Modelling&amp;rft.date=2006&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fhomepages.inf.ed.ac.uk%2Fballison%2Fpdf%2Flrec_skipgrams.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span></li>"
"</ol>"
"</div>"
"<ul>"
"<li>Christopher D. Manning, Hinrich Schütze, <i>Foundations of Statistical Natural Language Processing</i>, MIT Press: 1999. <a href=\"/wiki/Special:BookSources/0262133601\" class=\"internal mw-magiclink-isbn\">ISBN 0-262-13360-1</a>.</li>"
"<li>Owen White, Ted Dunning, Granger Sutton, Mark Adams, J.Craig Venter, and Chris Fields. A quality control algorithm for dna sequencing projects. Nucleic Acids Research, 21(16):3829—3838, 1993.</li>"
"<li>Frederick J. Damerau, <i>Markov Models and Linguistic Theory</i>. Mouton. The Hague, 1971.</li>"
"<li><span class=\"citation conference\">Brocardo, Marcelo Luiz; Issa Traore, Sherif Saad, Isaac Woungang (2013). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.uvic.ca/engineering/ece/isot/assets/docs/Authorship_Verification_for_Short_Messages_using_Stylometry.pdf\">\"Authorship Verification for Short Messages Using Stylometry\"</a>. IEEE Intl. Conference on Computer, Information and Telecommunication Systems (CITS).</span><span title=\"ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram&amp;rft.au=Brocardo%2C+Marcelo+Luiz&amp;rft.aufirst=Marcelo+Luiz&amp;rft.aulast=Brocardo&amp;rft.btitle=Authorship+Verification+for+Short+Messages+Using+Stylometry&amp;rft.date=2013&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fwww.uvic.ca%2Fengineering%2Fece%2Fisot%2Fassets%2Fdocs%2FAuthorship_Verification_for_Short_Messages_using_Stylometry.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span> <span style=\"display:none;font-size:100%\" class=\"error citation-comment\">Cite uses deprecated parameters (<a href=\"/wiki/Help:CS1_errors#deprecated_params\" title=\"Help:CS1 errors\">help</a>)</span></li>"
"</ul>"
"<h2><span class=\"mw-headline\" id=\"External_links\">External links</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=12\" title=\"Edit section: External links\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>"
"<ul>"
"<li><a rel=\"nofollow\" class=\"external text\" href=\"http://ngrams.googlelabs.com/\">Google's Google Book n-gram viewer</a> and <a rel=\"nofollow\" class=\"external text\" href=\"http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html\">Web n-grams database</a> (September 2006)</li>"
"<li><a rel=\"nofollow\" class=\"external text\" href=\"http://research.microsoft.com/web-ngram\">Microsoft's web n-grams service</a></li>"
"<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.ngrams.info/\">1,000,000 most frequent 2,3,4,5-grams from the 425 million word</a> <a href=\"/wiki/Corpus_of_Contemporary_American_English\" title=\"Corpus of Contemporary American English\">Corpus of Contemporary American English</a></li>"
"<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.peachnote.com/\">Peachnote's music ngram viewer</a></li>"
"<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.w3.org/TR/ngram-spec/\">Stochastic Language Models (N-Gram) Specification</a> (W3C)</li>"
"<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/lm.pdf\">Michael Collin's notes on N-Gram Language Models</a></li>"
"</ul>"
"<!-- "
"NewPP limit report"
"Parsed by mw1139"
"CPU time usage: 0.812 seconds"
"Real time usage: 0.889 seconds"
"Preprocessor visited node count: 998/1000000"
"Preprocessor generated node count: 3449/1500000"
"Post‐expand include size: 19942/2048000 bytes"
"Template argument size: 2595/2048000 bytes"
"Highest expansion depth: 11/40"
"Expensive parser function count: 5/500"
"Lua time usage: 0.097/10.000 seconds"
"Lua memory usage: 1.91 MB/50 MB"
"-->"
"<!-- Saved in parser cache with key enwiki:pcache:idhash:986182-0!*!0!!en!4!*!math=0 and timestamp 20140630152248 and revision id 615028208"
" -->"
"<noscript><img src=\"//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1\" alt=\"\" title=\"\" width=\"1\" height=\"1\" style=\"border: none; position: absolute;\" /></noscript></div>									<div class=\"printfooter\">"
"						Retrieved from \"<a dir=\"ltr\" href=\"http://en.wikipedia.org/w/index.php?title=N-gram&amp;oldid=615028208\">http://en.wikipedia.org/w/index.php?title=N-gram&amp;oldid=615028208</a>\"					</div>"
"													<div id='catlinks' class='catlinks'><div id=\"mw-normal-catlinks\" class=\"mw-normal-catlinks\"><a href=\"/wiki/Help:Category\" title=\"Help:Category\">Categories</a>: <ul><li><a href=\"/wiki/Category:Natural_language_processing\" title=\"Category:Natural language processing\">Natural language processing</a></li><li><a href=\"/wiki/Category:Computational_linguistics\" title=\"Category:Computational linguistics\">Computational linguistics</a></li><li><a href=\"/wiki/Category:Language_modeling\" title=\"Category:Language modeling\">Language modeling</a></li><li><a href=\"/wiki/Category:Speech_recognition\" title=\"Category:Speech recognition\">Speech recognition</a></li><li><a href=\"/wiki/Category:Corpus_linguistics\" title=\"Category:Corpus linguistics\">Corpus linguistics</a></li><li><a href=\"/wiki/Category:Probabilistic_models\" title=\"Category:Probabilistic models\">Probabilistic models</a></li></ul></div><div id=\"mw-hidden-catlinks\" class=\"mw-hidden-catlinks mw-hidden-cats-hidden\">Hidden categories: <ul><li><a href=\"/wiki/Category:Articles_lacking_in-text_citations_from_February_2011\" title=\"Category:Articles lacking in-text citations from February 2011\">Articles lacking in-text citations from February 2011</a></li><li><a href=\"/wiki/Category:All_articles_lacking_in-text_citations\" title=\"Category:All articles lacking in-text citations\">All articles lacking in-text citations</a></li><li><a href=\"/wiki/Category:All_articles_with_specifically_marked_weasel-worded_phrases\" title=\"Category:All articles with specifically marked weasel-worded phrases\">All articles with specifically marked weasel-worded phrases</a></li><li><a href=\"/wiki/Category:Articles_with_specifically_marked_weasel-worded_phrases_from_November_2011\" title=\"Category:Articles with specifically marked weasel-worded phrases from November 2011\">Articles with specifically marked weasel-worded phrases from November 2011</a></li><li><a href=\"/wiki/Category:Articles_with_specifically_marked_weasel-worded_phrases_from_June_2014\" title=\"Category:Articles with specifically marked weasel-worded phrases from June 2014\">Articles with specifically marked weasel-worded phrases from June 2014</a></li><li><a href=\"/wiki/Category:All_articles_with_unsourced_statements\" title=\"Category:All articles with unsourced statements\">All articles with unsourced statements</a></li><li><a href=\"/wiki/Category:Articles_with_unsourced_statements_from_November_2011\" title=\"Category:Articles with unsourced statements from November 2011\">Articles with unsourced statements from November 2011</a></li><li><a href=\"/wiki/Category:Pages_containing_cite_templates_with_deprecated_parameters\" title=\"Category:Pages containing cite templates with deprecated parameters\">Pages containing cite templates with deprecated parameters</a></li></ul></div></div>												<div class=\"visualClear\"></div>"
"							</div>"
"		</div>"
"		<div id=\"mw-navigation\">"
"			<h2>Navigation menu</h2>"
"			<div id=\"mw-head\">"
"									<div id=\"p-personal\" role=\"navigation\" class=\"\" aria-labelledby=\"p-personal-label\">"
"						<h3 id=\"p-personal-label\">Personal tools</h3>"
"						<ul>"
"							<li id=\"pt-createaccount\"><a href=\"/w/index.php?title=Special:UserLogin&amp;returnto=N-gram&amp;type=signup\">Create account</a></li><li id=\"pt-login\"><a href=\"/w/index.php?title=Special:UserLogin&amp;returnto=N-gram\" title=\"You're encouraged to log in; however, it's not mandatory. [o]\" accesskey=\"o\">Log in</a></li>						</ul>"
"					</div>"
"									<div id=\"left-navigation\">"
"										<div id=\"p-namespaces\" role=\"navigation\" class=\"vectorTabs\" aria-labelledby=\"p-namespaces-label\">"
"						<h3 id=\"p-namespaces-label\">Namespaces</h3>"
"						<ul>"
"															<li  id=\"ca-nstab-main\" class=\"selected\"><span><a href=\"/wiki/N-gram\"  title=\"View the content page [c]\" accesskey=\"c\">Article</a></span></li>"
"															<li  id=\"ca-talk\"><span><a href=\"/wiki/Talk:N-gram\"  title=\"Discussion about the content page [t]\" accesskey=\"t\">Talk</a></span></li>"
"													</ul>"
"					</div>"
"										<div id=\"p-variants\" role=\"navigation\" class=\"vectorMenu emptyPortlet\" aria-labelledby=\"p-variants-label\">"
"						<h3 id=\"mw-vector-current-variant\">"
"													</h3>"
"						<h3 id=\"p-variants-label\"><span>Variants</span><a href=\"#\"></a></h3>"
"						<div class=\"menu\">"
"							<ul>"
"															</ul>"
"						</div>"
"					</div>"
"									</div>"
"				<div id=\"right-navigation\">"
"										<div id=\"p-views\" role=\"navigation\" class=\"vectorTabs\" aria-labelledby=\"p-views-label\">"
"						<h3 id=\"p-views-label\">Views</h3>"
"						<ul>"
"															<li id=\"ca-view\" class=\"selected\"><span><a href=\"/wiki/N-gram\" >Read</a></span></li>"
"															<li id=\"ca-edit\"><span><a href=\"/w/index.php?title=N-gram&amp;action=edit\"  title=\"You can edit this page. Please use the preview button before saving [e]\" accesskey=\"e\">Edit</a></span></li>"
"															<li id=\"ca-history\" class=\"collapsible\"><span><a href=\"/w/index.php?title=N-gram&amp;action=history\"  title=\"Past versions of this page [h]\" accesskey=\"h\">View history</a></span></li>"
"													</ul>"
"					</div>"
"										<div id=\"p-cactions\" role=\"navigation\" class=\"vectorMenu emptyPortlet\" aria-labelledby=\"p-cactions-label\">"
"						<h3 id=\"p-cactions-label\"><span>More</span><a href=\"#\"></a></h3>"
"						<div class=\"menu\">"
"							<ul>"
"															</ul>"
"						</div>"
"					</div>"
"										<div id=\"p-search\" role=\"search\">"
"						<h3>"
"							<label for=\"searchInput\">Search</label>"
"						</h3>"
"						<form action=\"/w/index.php\" id=\"searchform\">"
"														<div id=\"simpleSearch\">"
"															<input type=\"search\" name=\"search\" placeholder=\"Search\" title=\"Search Wikipedia [f]\" accesskey=\"f\" id=\"searchInput\" /><input type=\"hidden\" value=\"Special:Search\" name=\"title\" /><input type=\"submit\" name=\"fulltext\" value=\"Search\" title=\"Search Wikipedia for this text\" id=\"mw-searchButton\" class=\"searchButton mw-fallbackSearchButton\" /><input type=\"submit\" name=\"go\" value=\"Go\" title=\"Go to a page with this exact name if one exists\" id=\"searchButton\" class=\"searchButton\" />								</div>"
"						</form>"
"					</div>"
"									</div>"
"			</div>"
"			<div id=\"mw-panel\">"
"				<div id=\"p-logo\" role=\"banner\"><a style=\"background-image: url(//upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);\" href=\"/wiki/Main_Page\"  title=\"Visit the main page\"></a></div>"
"						<div class=\"portal\" role=\"navigation\" id='p-navigation' aria-labelledby='p-navigation-label'>"
"			<h3 id='p-navigation-label'>Navigation</h3>"
"			<div class=\"body\">"
"									<ul>"
"													<li id=\"n-mainpage-description\"><a href=\"/wiki/Main_Page\" title=\"Visit the main page [z]\" accesskey=\"z\">Main page</a></li>"
"													<li id=\"n-contents\"><a href=\"/wiki/Portal:Contents\" title=\"Guides to browsing Wikipedia\">Contents</a></li>"
"													<li id=\"n-featuredcontent\"><a href=\"/wiki/Portal:Featured_content\" title=\"Featured content – the best of Wikipedia\">Featured content</a></li>"
"													<li id=\"n-currentevents\"><a href=\"/wiki/Portal:Current_events\" title=\"Find background information on current events\">Current events</a></li>"
"													<li id=\"n-randompage\"><a href=\"/wiki/Special:Random\" title=\"Load a random article [x]\" accesskey=\"x\">Random article</a></li>"
"													<li id=\"n-sitesupport\"><a href=\"https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en\" title=\"Support us\">Donate to Wikipedia</a></li>"
"													<li id=\"n-shoplink\"><a href=\"//shop.wikimedia.org\" title=\"Visit the Wikimedia Shop\">Wikimedia Shop</a></li>"
"											</ul>"
"							</div>"
"		</div>"
"			<div class=\"portal\" role=\"navigation\" id='p-interaction' aria-labelledby='p-interaction-label'>"
"			<h3 id='p-interaction-label'>Interaction</h3>"
"			<div class=\"body\">"
"									<ul>"
"													<li id=\"n-help\"><a href=\"/wiki/Help:Contents\" title=\"Guidance on how to use and edit Wikipedia\">Help</a></li>"
"													<li id=\"n-aboutsite\"><a href=\"/wiki/Wikipedia:About\" title=\"Find out about Wikipedia\">About Wikipedia</a></li>"
"													<li id=\"n-portal\"><a href=\"/wiki/Wikipedia:Community_portal\" title=\"About the project, what you can do, where to find things\">Community portal</a></li>"
"													<li id=\"n-recentchanges\"><a href=\"/wiki/Special:RecentChanges\" title=\"A list of recent changes in the wiki [r]\" accesskey=\"r\">Recent changes</a></li>"
"													<li id=\"n-contactpage\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\">Contact page</a></li>"
"											</ul>"
"							</div>"
"		</div>"
"			<div class=\"portal\" role=\"navigation\" id='p-tb' aria-labelledby='p-tb-label'>"
"			<h3 id='p-tb-label'>Tools</h3>"
"			<div class=\"body\">"
"									<ul>"
"													<li id=\"t-whatlinkshere\"><a href=\"/wiki/Special:WhatLinksHere/N-gram\" title=\"List of all English Wikipedia pages containing links to this page [j]\" accesskey=\"j\">What links here</a></li>"
"													<li id=\"t-recentchangeslinked\"><a href=\"/wiki/Special:RecentChangesLinked/N-gram\" title=\"Recent changes in pages linked from this page [k]\" accesskey=\"k\">Related changes</a></li>"
"													<li id=\"t-upload\"><a href=\"/wiki/Wikipedia:File_Upload_Wizard\" title=\"Upload files [u]\" accesskey=\"u\">Upload file</a></li>"
"													<li id=\"t-specialpages\"><a href=\"/wiki/Special:SpecialPages\" title=\"A list of all special pages [q]\" accesskey=\"q\">Special pages</a></li>"
"													<li id=\"t-permalink\"><a href=\"/w/index.php?title=N-gram&amp;oldid=615028208\" title=\"Permanent link to this revision of the page\">Permanent link</a></li>"
"													<li id=\"t-info\"><a href=\"/w/index.php?title=N-gram&amp;action=info\">Page information</a></li>"
"													<li id=\"t-wikibase\"><a href=\"//www.wikidata.org/wiki/Q94489\" title=\"Link to connected data repository item [g]\" accesskey=\"g\">Data item</a></li>"
"						<li id=\"t-cite\"><a href=\"/w/index.php?title=Special:Cite&amp;page=N-gram&amp;id=615028208\" title=\"Information on how to cite this page\">Cite this page</a></li>					</ul>"
"							</div>"
"		</div>"
"			<div class=\"portal\" role=\"navigation\" id='p-coll-print_export' aria-labelledby='p-coll-print_export-label'>"
"			<h3 id='p-coll-print_export-label'>Print/export</h3>"
"			<div class=\"body\">"
"									<ul>"
"													<li id=\"coll-create_a_book\"><a href=\"/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=N-gram\">Create a book</a></li>"
"													<li id=\"coll-download-as-rl\"><a href=\"/w/index.php?title=Special:Book&amp;bookcmd=render_article&amp;arttitle=N-gram&amp;oldid=615028208&amp;writer=rl\">Download as PDF</a></li>"
"													<li id=\"t-print\"><a href=\"/w/index.php?title=N-gram&amp;printable=yes\" title=\"Printable version of this page [p]\" accesskey=\"p\">Printable version</a></li>"
"											</ul>"
"							</div>"
"		</div>"
"			<div class=\"portal\" role=\"navigation\" id='p-lang' aria-labelledby='p-lang-label'>"
"			<h3 id='p-lang-label'>Languages</h3>"
"			<div class=\"body\">"
"									<ul>"
"													<li class=\"interlanguage-link interwiki-ca\"><a href=\"//ca.wikipedia.org/wiki/N-grama\" title=\"N-grama – Catalan\" lang=\"ca\" hreflang=\"ca\">Català</a></li>"
"													<li class=\"interlanguage-link interwiki-cs\"><a href=\"//cs.wikipedia.org/wiki/N-gram\" title=\"N-gram – Czech\" lang=\"cs\" hreflang=\"cs\">Čeština</a></li>"
"													<li class=\"interlanguage-link interwiki-de\"><a href=\"//de.wikipedia.org/wiki/N-Gramm\" title=\"N-Gramm – German\" lang=\"de\" hreflang=\"de\">Deutsch</a></li>"
"													<li class=\"interlanguage-link interwiki-es\"><a href=\"//es.wikipedia.org/wiki/N-grama\" title=\"N-grama – Spanish\" lang=\"es\" hreflang=\"es\">Español</a></li>"
"													<li class=\"interlanguage-link interwiki-eu\"><a href=\"//eu.wikipedia.org/wiki/N-grama\" title=\"N-grama – Basque\" lang=\"eu\" hreflang=\"eu\">Euskara</a></li>"
"													<li class=\"interlanguage-link interwiki-fr\"><a href=\"//fr.wikipedia.org/wiki/N-gramme\" title=\"N-gramme – French\" lang=\"fr\" hreflang=\"fr\">Français</a></li>"
"													<li class=\"interlanguage-link interwiki-it\"><a href=\"//it.wikipedia.org/wiki/N-gramma\" title=\"N-gramma – Italian\" lang=\"it\" hreflang=\"it\">Italiano</a></li>"
"													<li class=\"interlanguage-link interwiki-mhr\"><a href=\"//mhr.wikipedia.org/wiki/N-%D0%B3%D1%80%D0%B0%D0%BC\" title=\"N-грам – Eastern Mari\" lang=\"mhr\" hreflang=\"mhr\">Олык марий</a></li>"
"													<li class=\"interlanguage-link interwiki-pl\"><a href=\"//pl.wikipedia.org/wiki/N-gram\" title=\"N-gram – Polish\" lang=\"pl\" hreflang=\"pl\">Polski</a></li>"
"													<li class=\"interlanguage-link interwiki-ru\"><a href=\"//ru.wikipedia.org/wiki/N-%D0%B3%D1%80%D0%B0%D0%BC%D0%BC\" title=\"N-грамм – Russian\" lang=\"ru\" hreflang=\"ru\">Русский</a></li>"
"													<li class=\"interlanguage-link interwiki-sk\"><a href=\"//sk.wikipedia.org/wiki/N-gram\" title=\"N-gram – Slovak\" lang=\"sk\" hreflang=\"sk\">Slovenčina</a></li>"
"													<li class=\"interlanguage-link interwiki-fi\"><a href=\"//fi.wikipedia.org/wiki/N-grammi\" title=\"N-grammi – Finnish\" lang=\"fi\" hreflang=\"fi\">Suomi</a></li>"
"													<li class=\"uls-p-lang-dummy\"><a href=\"#\"></a></li>"
"											</ul>"
"				<div class='after-portlet after-portlet-lang'><span class=\"wb-langlinks-edit wb-langlinks-link\"><a action=\"edit\" href=\"//www.wikidata.org/wiki/Q94489#sitelinks-wikipedia\" text=\"Edit links\" title=\"Edit interlanguage links\" class=\"wbc-editpage\">Edit links</a></span></div>			</div>"
"		</div>"
"				</div>"
"		</div>"
"		<div id=\"footer\" role=\"contentinfo\">"
"							<ul id=\"footer-info\">"
"											<li id=\"footer-info-lastmod\"> This page was last modified on 30 June 2014 at 15:22.<br /></li>"
"											<li id=\"footer-info-copyright\">Text is available under the <a rel=\"license\" href=\"//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\">Creative Commons Attribution-ShareAlike License</a><a rel=\"license\" href=\"//creativecommons.org/licenses/by-sa/3.0/\" style=\"display:none;\"></a>;"
"additional terms may apply.  By using this site, you agree to the <a href=\"//wikimediafoundation.org/wiki/Terms_of_Use\">Terms of Use</a> and <a href=\"//wikimediafoundation.org/wiki/Privacy_policy\">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href=\"//www.wikimediafoundation.org/\">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>"
"									</ul>"
"							<ul id=\"footer-places\">"
"											<li id=\"footer-places-privacy\"><a href=\"//wikimediafoundation.org/wiki/Privacy_policy\" title=\"wikimedia:Privacy policy\">Privacy policy</a></li>"
"											<li id=\"footer-places-about\"><a href=\"/wiki/Wikipedia:About\" title=\"Wikipedia:About\">About Wikipedia</a></li>"
"											<li id=\"footer-places-disclaimer\"><a href=\"/wiki/Wikipedia:General_disclaimer\" title=\"Wikipedia:General disclaimer\">Disclaimers</a></li>"
"											<li id=\"footer-places-contact\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\">Contact Wikipedia</a></li>"
"											<li id=\"footer-places-developers\"><a href=\"https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute\">Developers</a></li>"
"											<li id=\"footer-places-mobileview\"><a href=\"//en.m.wikipedia.org/wiki/N-gram\" class=\"noprint stopMobileRedirectToggle\">Mobile view</a></li>"
"									</ul>"
"										<ul id=\"footer-icons\" class=\"noprint\">"
"											<li id=\"footer-copyrightico\">"
"															<a href=\"//wikimediafoundation.org/\"><img src=\"//bits.wikimedia.org/images/wikimedia-button.png\" width=\"88\" height=\"31\" alt=\"Wikimedia Foundation\"/></a>"
"													</li>"
"											<li id=\"footer-poweredbyico\">"
"															<a href=\"//www.mediawiki.org/\"><img src=\"//bits.wikimedia.org/static-1.24wmf10/skins/common/images/poweredby_mediawiki_88x31.png\" alt=\"Powered by MediaWiki\" width=\"88\" height=\"31\" /></a>"
"													</li>"
"									</ul>"
"						<div style=\"clear:both\"></div>"
"		</div>"
"		<script>/*<![CDATA[*/window.jQuery && jQuery.ready();/*]]>*/</script><script>if(window.mw){"
"mw.loader.state({\"site\":\"loading\",\"user\":\"ready\",\"user.groups\":\"ready\"});"
"}</script>"
"<script>if(window.mw){"
"mw.loader.load([\"ext.math.mathjax.enabler\",\"ext.cite\",\"mediawiki.toc\",\"mobile.desktop\",\"mediawiki.action.view.postEdit\",\"mediawiki.user\",\"mediawiki.hidpi\",\"mediawiki.page.ready\",\"mediawiki.searchSuggest\",\"ext.gadget.teahouse\",\"ext.gadget.ReferenceTooltips\",\"ext.gadget.DRN-wizard\",\"ext.gadget.charinsert\",\"ext.gadget.refToolbar\",\"mw.MwEmbedSupport.style\",\"mmv.bootstrap.autostart\",\"ext.eventLogging.subscriber\",\"ext.navigationTiming\",\"schema.UniversalLanguageSelector\",\"ext.uls.eventlogger\",\"ext.uls.interlanguage\"],null,true);"
"}</script>"
"<script src=\"//bits.wikimedia.org/en.wikipedia.org/load.php?debug=false&amp;lang=en&amp;modules=site&amp;only=scripts&amp;skin=vector&amp;*\"></script>"
"<script>if(window.mw){"
"mw.config.set({\"wgBackendResponseTime\":254,\"wgHostname\":\"mw1023\"});"
"}</script>"
"	</body>"
"</html>"
"	"
