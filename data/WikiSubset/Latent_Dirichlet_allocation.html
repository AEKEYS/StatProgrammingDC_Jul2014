"<!DOCTYPE html>"
"<html lang=\"en\" dir=\"ltr\" class=\"client-nojs\">"
"<head>"
"<meta charset=\"UTF-8\" />"
"<title>Latent Dirichlet allocation - Wikipedia, the free encyclopedia</title>"
"<meta name=\"generator\" content=\"MediaWiki 1.24wmf10\" />"
"<link rel=\"alternate\" href=\"android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Latent_Dirichlet_allocation\" />"
"<link rel=\"alternate\" type=\"application/x-wiki\" title=\"Edit this page\" href=\"/w/index.php?title=Latent_Dirichlet_allocation&amp;action=edit\" />"
"<link rel=\"edit\" title=\"Edit this page\" href=\"/w/index.php?title=Latent_Dirichlet_allocation&amp;action=edit\" />"
"<link rel=\"apple-touch-icon\" href=\"//bits.wikimedia.org/apple-touch/wikipedia.png\" />"
"<link rel=\"shortcut icon\" href=\"//bits.wikimedia.org/favicon/wikipedia.ico\" />"
"<link rel=\"search\" type=\"application/opensearchdescription+xml\" href=\"/w/opensearch_desc.php\" title=\"Wikipedia (en)\" />"
"<link rel=\"EditURI\" type=\"application/rsd+xml\" href=\"//en.wikipedia.org/w/api.php?action=rsd\" />"
"<link rel=\"copyright\" href=\"//creativecommons.org/licenses/by-sa/3.0/\" />"
"<link rel=\"alternate\" type=\"application/atom+xml\" title=\"Wikipedia Atom feed\" href=\"/w/index.php?title=Special:RecentChanges&amp;feed=atom\" />"
"<link rel=\"canonical\" href=\"http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\" />"
"<link rel=\"stylesheet\" href=\"//bits.wikimedia.org/en.wikipedia.org/load.php?debug=false&amp;lang=en&amp;modules=ext.gadget.DRN-wizard%2CReferenceTooltips%2Ccharinsert%2CrefToolbar%2Cteahouse%7Cext.math.styles%7Cext.rtlcite%2Cwikihiero%7Cext.uls.nojs%7Cext.visualEditor.viewPageTarget.noscript%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.skinning.interface%7Cmediawiki.ui.button%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector&amp;*\" />"
"<meta name=\"ResourceLoaderDynamicStyles\" content=\"\" />"
"<link rel=\"stylesheet\" href=\"//bits.wikimedia.org/en.wikipedia.org/load.php?debug=false&amp;lang=en&amp;modules=site&amp;only=styles&amp;skin=vector&amp;*\" />"
"<style>a:lang(ar),a:lang(kk-arab),a:lang(mzn),a:lang(ps),a:lang(ur){text-decoration:none}"
"/* cache key: enwiki:resourceloader:filter:minify-css:7:3904d24a08aa08f6a68dc338f9be277e */</style>"
"<script src=\"//bits.wikimedia.org/en.wikipedia.org/load.php?debug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector&amp;*\"></script>"
"<script>if(window.mw){"
"mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Latent_Dirichlet_allocation\",\"wgTitle\":\"Latent Dirichlet allocation\",\"wgCurRevisionId\":610319663,\"wgRevisionId\":610319663,\"wgArticleId\":4605351,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Pages containing cite templates with deprecated parameters\",\"All articles with unsourced statements\",\"Articles with unsourced statements from September 2013\",\"Statistical natural language processing\",\"Latent variable models\",\"Probabilistic models\"],\"wgBreakFrames\":false,\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgMonthNamesShort\":[\"\",\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"],\"wgRelevantPageName\":\"Latent_Dirichlet_allocation\",\"wgIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgWikiEditorEnabledModules\":{\"toolbar\":true,\"dialogs\":true,\"hidesig\":true,\"preview\":false,\"previewDialog\":false,\"publish\":false},\"wgBetaFeaturesFeatures\":[],\"wgMediaViewerOnClick\":true,\"wgVisualEditor\":{\"isPageWatched\":false,\"magnifyClipIconURL\":\"//bits.wikimedia.org/static-1.24wmf10/skins/common/images/magnify-clip.png\",\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"svgMaxSize\":2048,\"namespacesWithSubpages\":{\"6\":0,\"8\":0,\"1\":true,\"2\":true,\"3\":true,\"4\":true,\"5\":true,\"7\":true,\"9\":true,\"10\":true,\"11\":true,\"12\":true,\"13\":true,\"14\":true,\"15\":true,\"100\":true,\"101\":true,\"102\":true,\"103\":true,\"104\":true,\"105\":true,\"106\":true,\"107\":true,\"108\":true,\"109\":true,\"110\":true,\"111\":true,\"447\":true,\"828\":true,\"829\":true}},\"wikilove-recipient\":\"\",\"wikilove-anon\":0,\"wgGuidedTourHelpGuiderUrl\":\"Help:Guided tours/guider\",\"wgFlowTermsOfUseEdit\":\"By saving changes, you agree to our \u003Ca class=\\"external text\\" href=\\"//wikimediafoundation.org/wiki/Terms_of_use\\"\u003ETerms of Use\u003C/a\u003E and agree to irrevocably release your text under the \u003Ca rel=\\"nofollow\\" class=\\"external text\\" href=\\"//creativecommons.org/licenses/by-sa/3.0\\"\u003ECC BY-SA 3.0 License\u003C/a\u003E and \u003Ca class=\\"external text\\" href=\\"//en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License\\"\u003EGFDL\u003C/a\u003E\",\"wgFlowTermsOfUseSummarize\":\"By clicking \\"Summarize\\", you agree to the terms of use for this wiki.\",\"wgFlowTermsOfUseCloseTopic\":\"By clicking \\"Close topic\\", you agree to the terms of use for this wiki.\",\"wgFlowTermsOfUseReopenTopic\":\"By clicking \\"Reopen topic\\", you agree to the terms of use for this wiki.\",\"wgULSAcceptLanguageList\":[\"en-us\"],\"wgULSCurrentAutonym\":\"English\",\"wgFlaggedRevsParams\":{\"tags\":{\"status\":{\"levels\":1,\"quality\":2,\"pristine\":3}}},\"wgStableRevisionId\":null,\"wgCategoryTreePageCategoryOptions\":\"{\\"mode\\":0,\\"hideprefix\\":20,\\"showcount\\":true,\\"namespaces\\":false}\",\"wgNoticeProject\":\"wikipedia\",\"wgWikibaseItemId\":\"Q269236\"});"
"}</script><script>if(window.mw){"
"mw.loader.implement(\"user.options\",function($,jQuery){mw.user.options.set({\"ccmeonemails\":0,\"cols\":80,\"date\":\"default\",\"diffonly\":0,\"disablemail\":0,\"editfont\":\"default\",\"editondblclick\":0,\"editsectiononrightclick\":0,\"enotifminoredits\":0,\"enotifrevealaddr\":0,\"enotifusertalkpages\":1,\"enotifwatchlistpages\":0,\"extendwatchlist\":0,\"fancysig\":0,\"forceeditsummary\":0,\"gender\":\"unknown\",\"hideminor\":0,\"hidepatrolled\":0,\"imagesize\":2,\"math\":0,\"minordefault\":0,\"newpageshidepatrolled\":0,\"nickname\":\"\",\"norollbackdiff\":0,\"numberheadings\":0,\"previewonfirst\":0,\"previewontop\":1,\"rcdays\":7,\"rclimit\":50,\"rows\":25,\"showhiddencats\":false,\"shownumberswatching\":1,\"showtoolbar\":1,\"skin\":\"vector\",\"stubthreshold\":0,\"thumbsize\":4,\"underline\":2,\"uselivepreview\":0,\"usenewrc\":0,\"watchcreations\":1,\"watchdefault\":0,\"watchdeletion\":0,\"watchlistdays\":3,\"watchlisthideanons\":0,\"watchlisthidebots\":0,\"watchlisthideliu\":0,\"watchlisthideminor\":0,\"watchlisthideown\":0,\"watchlisthidepatrolled\":0,\"watchmoves\":0,\"wllimit\":250,"
"\"useeditwarning\":1,\"prefershttps\":1,\"flaggedrevssimpleui\":1,\"flaggedrevsstable\":0,\"flaggedrevseditdiffs\":true,\"flaggedrevsviewdiffs\":false,\"usebetatoolbar\":1,\"usebetatoolbar-cgd\":1,\"multimediaviewer-enable\":true,\"visualeditor-enable\":0,\"visualeditor-betatempdisable\":0,\"visualeditor-enable-experimental\":0,\"visualeditor-enable-language\":0,\"visualeditor-hidebetawelcome\":0,\"wikilove-enabled\":1,\"mathJax\":false,\"echo-subscriptions-web-page-review\":true,\"echo-subscriptions-email-page-review\":false,\"ep_showtoplink\":false,\"ep_bulkdelorgs\":false,\"ep_bulkdelcourses\":true,\"ep_showdyk\":true,\"echo-subscriptions-web-education-program\":true,\"echo-subscriptions-email-education-program\":false,\"echo-notify-show-link\":true,\"echo-show-alert\":true,\"echo-email-frequency\":0,\"echo-email-format\":\"html\",\"echo-subscriptions-email-system\":true,\"echo-subscriptions-web-system\":true,\"echo-subscriptions-email-user-rights\":true,\"echo-subscriptions-web-user-rights\":true,\"echo-subscriptions-email-other\":false,"
"\"echo-subscriptions-web-other\":true,\"echo-subscriptions-email-edit-user-talk\":false,\"echo-subscriptions-web-edit-user-talk\":true,\"echo-subscriptions-email-reverted\":false,\"echo-subscriptions-web-reverted\":true,\"echo-subscriptions-email-article-linked\":false,\"echo-subscriptions-web-article-linked\":false,\"echo-subscriptions-email-mention\":false,\"echo-subscriptions-web-mention\":true,\"echo-subscriptions-web-edit-thank\":true,\"echo-subscriptions-email-edit-thank\":false,\"echo-subscriptions-web-flow-discussion\":true,\"echo-subscriptions-email-flow-discussion\":false,\"gettingstarted-task-toolbar-show-intro\":true,\"uls-preferences\":\"\",\"language\":\"en\",\"variant-gan\":\"gan\",\"variant-iu\":\"iu\",\"variant-kk\":\"kk\",\"variant-ku\":\"ku\",\"variant-shi\":\"shi\",\"variant-sr\":\"sr\",\"variant-tg\":\"tg\",\"variant-uz\":\"uz\",\"variant-zh\":\"zh\",\"searchNs0\":true,\"searchNs1\":false,\"searchNs2\":false,\"searchNs3\":false,\"searchNs4\":false,\"searchNs5\":false,\"searchNs6\":false,\"searchNs7\":false,\"searchNs8\":false,\"searchNs9\":false,"
"\"searchNs10\":false,\"searchNs11\":false,\"searchNs12\":false,\"searchNs13\":false,\"searchNs14\":false,\"searchNs15\":false,\"searchNs100\":false,\"searchNs101\":false,\"searchNs108\":false,\"searchNs109\":false,\"searchNs118\":false,\"searchNs119\":false,\"searchNs446\":false,\"searchNs447\":false,\"searchNs710\":false,\"searchNs711\":false,\"searchNs828\":false,\"searchNs829\":false,\"gadget-teahouse\":1,\"gadget-ReferenceTooltips\":1,\"gadget-DRN-wizard\":1,\"gadget-charinsert\":1,\"gadget-refToolbar\":1,\"gadget-mySandbox\":1,\"variant\":\"en\"});},{},{});mw.loader.implement(\"user.tokens\",function($,jQuery){mw.user.tokens.set({\"editToken\":\"+\\\",\"patrolToken\":false,\"watchToken\":false});},{},{});"
"/* cache key: enwiki:resourceloader:filter:minify-js:7:bcb89b2f32dd43f2bd4d7fbef2aef0d5 */"
"}</script>"
"<script>if(window.mw){"
"mw.loader.load([\"mediawiki.page.startup\",\"mediawiki.legacy.wikibits\",\"mediawiki.legacy.ajax\",\"ext.centralauth.centralautologin\",\"mmv.head\",\"ext.visualEditor.viewPageTarget.init\",\"ext.uls.init\",\"ext.uls.interface\",\"ext.centralNotice.bannerController\",\"skins.vector.js\"]);"
"}</script>"
"<link rel=\"dns-prefetch\" href=\"//meta.wikimedia.org\" />"
"<!--[if lt IE 7]><style type=\"text/css\">body{behavior:url(\"/w/static-1.24wmf10/skins/vector/csshover.min.htc\")}</style><![endif]-->"
"</head>"
"<body class=\"mediawiki ltr sitedir-ltr ns-0 ns-subject page-Latent_Dirichlet_allocation skin-vector action-view vector-animateLayout\">"
"		<div id=\"mw-page-base\" class=\"noprint\"></div>"
"		<div id=\"mw-head-base\" class=\"noprint\"></div>"
"		<div id=\"content\" class=\"mw-body\" role=\"main\">"
"			<a id=\"top\"></a>"
"							<div id=\"siteNotice\"><!-- CentralNotice --></div>"
"						<h1 id=\"firstHeading\" class=\"firstHeading\" lang=\"en\"><span dir=\"auto\">Latent Dirichlet allocation</span></h1>"
"						<div id=\"bodyContent\" class=\"mw-body-content\">"
"									<div id=\"siteSub\">From Wikipedia, the free encyclopedia</div>"
"								<div id=\"contentSub\"></div>"
"												<div id=\"jump-to-nav\" class=\"mw-jump\">"
"					Jump to:					<a href=\"#mw-navigation\">navigation</a>, 					<a href=\"#p-search\">search</a>"
"				</div>"
"				<div id=\"mw-content-text\" lang=\"en\" dir=\"ltr\" class=\"mw-content-ltr\"><div class=\"hatnote\">Not to be confused with <a href=\"/wiki/Linear_discriminant_analysis\" title=\"Linear discriminant analysis\">linear discriminant analysis</a>.</div>"
"<p>In <a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">natural language processing</a>, <b>latent Dirichlet allocation</b> (<b>LDA</b>) is a <a href=\"/wiki/Generative_model\" title=\"Generative model\">generative model</a> that allows sets of observations to be explained by <a href=\"/wiki/Latent_variable\" title=\"Latent variable\">unobserved</a> groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a <a href=\"/wiki/Topic_model\" title=\"Topic model\">topic model</a> and was first presented as a <a href=\"/wiki/Graphical_models\" title=\"Graphical models\" class=\"mw-redirect\">graphical model</a> for topic discovery by <a href=\"/wiki/David_Blei\" title=\"David Blei\">David Blei</a>, <a href=\"/wiki/Andrew_Ng\" title=\"Andrew Ng\">Andrew Ng</a>, and <a href=\"/wiki/Michael_I._Jordan\" title=\"Michael I. Jordan\">Michael Jordan</a> in 2003.<sup id=\"cite_ref-blei2003_1-0\" class=\"reference\"><a href=\"#cite_note-blei2003-1\"><span>[</span>1<span>]</span></a></sup></p>"
"<p></p>"
"<div id=\"toc\" class=\"toc\">"
"<div id=\"toctitle\">"
"<h2>Contents</h2>"
"</div>"
"<ul>"
"<li class=\"toclevel-1 tocsection-1\"><a href=\"#Topics_in_LDA\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">Topics in LDA</span></a></li>"
"<li class=\"toclevel-1 tocsection-2\"><a href=\"#Model\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Model</span></a>"
"<ul>"
"<li class=\"toclevel-2 tocsection-3\"><a href=\"#Mathematical_definition\"><span class=\"tocnumber\">2.1</span> <span class=\"toctext\">Mathematical definition</span></a></li>"
"</ul>"
"</li>"
"<li class=\"toclevel-1 tocsection-4\"><a href=\"#Inference\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">Inference</span></a></li>"
"<li class=\"toclevel-1 tocsection-5\"><a href=\"#Applications.2C_extensions_and_similar_techniques\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">Applications, extensions and similar techniques</span></a></li>"
"<li class=\"toclevel-1 tocsection-6\"><a href=\"#See_also\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">See also</span></a></li>"
"<li class=\"toclevel-1 tocsection-7\"><a href=\"#Notes\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">Notes</span></a></li>"
"<li class=\"toclevel-1 tocsection-8\"><a href=\"#External_links\"><span class=\"tocnumber\">7</span> <span class=\"toctext\">External links</span></a></li>"
"</ul>"
"</div>"
"<p></p>"
"<h2><span class=\"mw-headline\" id=\"Topics_in_LDA\">Topics in LDA</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Latent_Dirichlet_allocation&amp;action=edit&amp;section=1\" title=\"Edit section: Topics in LDA\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>"
"<p>In LDA, each document may be viewed as a <a href=\"/wiki/Mixture_model\" title=\"Mixture model\">mixture</a> of various topics. This is similar to <a href=\"/wiki/PLSA\" title=\"PLSA\" class=\"mw-redirect\">probabilistic latent semantic analysis</a> (pLSA), except that in LDA the topic distribution is assumed to have a <a href=\"/wiki/Dirichlet_distribution\" title=\"Dirichlet distribution\">Dirichlet</a> <a href=\"/wiki/Prior_probability\" title=\"Prior probability\">prior</a>. In practice, this results in more reasonable mixtures of topics in a document. It has been noted, however, that the <a href=\"/wiki/PLSA\" title=\"PLSA\" class=\"mw-redirect\">pLSA</a> model is equivalent to the LDA model under a uniform Dirichlet prior distribution.<sup id=\"cite_ref-2\" class=\"reference\"><a href=\"#cite_note-2\"><span>[</span>2<span>]</span></a></sup></p>"
"<p>For example, an LDA model might have topics that can be classified as <b>CAT_related</b> and <b>DOG_related</b>. A topic has probabilities of generating various words, such as <i>milk</i>, <i>meow</i>, and <i>kitten</i>, which can be classified and interpreted by the viewer as \"CAT_related\". Naturally, the word <i>cat</i> itself will have high probability given this topic. The <b>DOG_related</b> topic likewise has probabilities of generating each word: <i>puppy</i>, <i>bark</i>, and <i>bone</i> might have high probability. Words without special relevance, such as <i>the</i> (see <a href=\"/wiki/Function_word\" title=\"Function word\">function word</a>), will have roughly even probability between classes (or can be placed into a separate category). A topic is not strongly defined, neither <a href=\"/wiki/Semantics\" title=\"Semantics\">semantically</a> nor <a href=\"/wiki/Epistemology\" title=\"Epistemology\">epistemologically</a>. It is identified on the basis of <a href=\"/wiki/Supervised_learning\" title=\"Supervised learning\">supervised</a> labeling and (manual) pruning on the basis of their likelihood of co-occurrence. A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.</p>"
"<p>Each document is assumed to be characterized by a particular set of topics. This is akin to the standard <a href=\"/wiki/Bag_of_words_model\" title=\"Bag of words model\" class=\"mw-redirect\">bag of words model</a> assumption, and makes the individual words <a href=\"/wiki/De_Finetti%27s_theorem\" title=\"De Finetti's theorem\">exchangeable</a>.</p>"
"<h2><span class=\"mw-headline\" id=\"Model\">Model</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Latent_Dirichlet_allocation&amp;action=edit&amp;section=2\" title=\"Edit section: Model\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>"
"<div class=\"thumb tright\">"
"<div class=\"thumbinner\" style=\"width:252px;\"><a href=\"/wiki/File:Latent_Dirichlet_allocation.svg\" class=\"image\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Latent_Dirichlet_allocation.svg/250px-Latent_Dirichlet_allocation.svg.png\" width=\"250\" height=\"131\" class=\"thumbimage\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Latent_Dirichlet_allocation.svg/375px-Latent_Dirichlet_allocation.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Latent_Dirichlet_allocation.svg/500px-Latent_Dirichlet_allocation.svg.png 2x\" data-file-width=\"593\" data-file-height=\"311\" /></a>"
"<div class=\"thumbcaption\">"
"<div class=\"magnify\"><a href=\"/wiki/File:Latent_Dirichlet_allocation.svg\" class=\"internal\" title=\"Enlarge\"><img src=\"//bits.wikimedia.org/static-1.24wmf7/skins/common/images/magnify-clip.png\" width=\"15\" height=\"11\" alt=\"\" /></a></div>"
"<a href=\"/wiki/Plate_notation\" title=\"Plate notation\">Plate notation</a> representing the LDA model.</div>"
"</div>"
"</div>"
"<p>With <a href=\"/wiki/Plate_notation\" title=\"Plate notation\">plate notation</a>, the dependencies among the many variables can be captured concisely. The boxes are “plates” representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of documents, N the number of words in a document. Thus:</p>"
"<dl>"
"<dd><i>α</i> is the parameter of the Dirichlet prior on the per-document topic distributions,</dd>"
"<dd><i>β</i> is the parameter of the Dirichlet prior on the per-topic word distribution,</dd>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\"\theta_i\" src=\"//upload.wikimedia.org/math/a/2/0/a2007f38f1ea83f4931410d022f07cd4.png\" /> is the topic distribution for document <i>i</i>,</dd>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\"\phi_k\" src=\"//upload.wikimedia.org/math/6/8/e/68e9a469735dac533d26c97825b81015.png\" /> is the word distribution for topic <i>k</i>,</dd>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\"z_{ij}\" src=\"//upload.wikimedia.org/math/f/0/0/f0037957d17e36fd79fa9107a54d0ed0.png\" /> is the topic for the <i>j</i>th word in document <i>i</i>, and</dd>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\"w_{ij}\" src=\"//upload.wikimedia.org/math/d/4/c/d4ceb969c50979e2dfbb7fe3e83d7a24.png\" /> is the specific word.</dd>"
"</dl>"
"<div class=\"thumb tright\">"
"<div class=\"thumbinner\" style=\"width:253px;\"><a href=\"/wiki/File:Smoothed_LDA.png\" class=\"image\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Smoothed_LDA.png/251px-Smoothed_LDA.png\" width=\"251\" height=\"124\" class=\"thumbimage\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Smoothed_LDA.png/377px-Smoothed_LDA.png 1.5x, //upload.wikimedia.org/wikipedia/commons/4/4d/Smoothed_LDA.png 2x\" data-file-width=\"494\" data-file-height=\"244\" /></a>"
"<div class=\"thumbcaption\">"
"<div class=\"magnify\"><a href=\"/wiki/File:Smoothed_LDA.png\" class=\"internal\" title=\"Enlarge\"><img src=\"//bits.wikimedia.org/static-1.24wmf7/skins/common/images/magnify-clip.png\" width=\"15\" height=\"11\" alt=\"\" /></a></div>"
"Plate notation for smoothed LDA</div>"
"</div>"
"</div>"
"<p>The <img class=\"mwe-math-fallback-png-inline tex\" alt=\"w_{ij}\" src=\"//upload.wikimedia.org/math/d/4/c/d4ceb969c50979e2dfbb7fe3e83d7a24.png\" /> are the only <a href=\"/wiki/Observable_variable\" title=\"Observable variable\">observable variables</a>, and the other variables are <a href=\"/wiki/Latent_variable\" title=\"Latent variable\">latent variables</a>. Mostly, the basic LDA model will be extended to a smoothed version to gain better results .<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">[<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (September 2013)\">citation needed</span></a></i>]</sup> The plate notation is shown on the right, where <i>K</i> denotes the number of topics considered in the model and:</p>"
"<dl>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\"\phi\" src=\"//upload.wikimedia.org/math/7/f/2/7f20aa0b3691b496aec21cf356f63e04.png\" /> is a <i>K*V</i> (<i>V</i> is the dimension of the vocabulary) <a href=\"/wiki/Markov_matrix\" title=\"Markov matrix\" class=\"mw-redirect\">Markov matrix</a> each row of which denotes the word distribution of a topic.</dd>"
"</dl>"
"<p>The generative process behind is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. LDA assumes the following generative process for a corpus <img class=\"mwe-math-fallback-png-inline tex\" alt=\"D\" src=\"//upload.wikimedia.org/math/f/6/2/f623e75af30e62bbd73d6df5b50bb7b5.png\" /> consisting of <img class=\"mwe-math-fallback-png-inline tex\" alt=\"M\" src=\"//upload.wikimedia.org/math/6/9/6/69691c7bdcc3ce6d5d8a1361f22d04ac.png\" /> documents each of length <img class=\"mwe-math-fallback-png-inline tex\" alt=\"N_i\" src=\"//upload.wikimedia.org/math/b/4/a/b4ad80684ed94b4380f0d33ba54b91da.png\" />:</p>"
"<p>1. Choose <img class=\"mwe-math-fallback-png-inline tex\" alt=\" \theta_i \, \sim \, \mathrm{Dir}(\alpha) \" src=\"//upload.wikimedia.org/math/2/1/5/215904682d7e620423218dfab0ee481b.png\" />, where <img class=\"mwe-math-fallback-png-inline tex\" alt=\" i \in \{ 1,\dots,M \} \" src=\"//upload.wikimedia.org/math/a/c/f/acfa7d3fa8b5dede007f1d23ff9ac34d.png\" /> and <img class=\"mwe-math-fallback-png-inline tex\" alt=\" \mathrm{Dir}(\alpha) \" src=\"//upload.wikimedia.org/math/8/f/1/8f1ede906215d901d5b1c91c4fbb8916.png\" /> is the <a href=\"/wiki/Dirichlet_distribution\" title=\"Dirichlet distribution\">Dirichlet distribution</a> for parameter <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\alpha\" src=\"//upload.wikimedia.org/math/b/c/c/bccfc7022dfb945174d9bcebad2297bb.png\" /></p>"
"<p>2. Choose <img class=\"mwe-math-fallback-png-inline tex\" alt=\" \phi_k \, \sim \, \mathrm{Dir}(\beta) \" src=\"//upload.wikimedia.org/math/2/2/7/227dce575073bac6c0a3246880c5300a.png\" />, where <img class=\"mwe-math-fallback-png-inline tex\" alt=\" k \in \{ 1,\dots,K \} \" src=\"//upload.wikimedia.org/math/7/2/6/72642dd5cf05d3557823f432314382fe.png\" /></p>"
"<p>3. For each of the word positions <img class=\"mwe-math-fallback-png-inline tex\" alt=\"i, j\" src=\"//upload.wikimedia.org/math/e/e/8/ee813f0ede8664a8049b1b6720f03b60.png\" />, where <img class=\"mwe-math-fallback-png-inline tex\" alt=\" j \in \{ 1,\dots,N_i \} \" src=\"//upload.wikimedia.org/math/5/5/1/5515ad87a137c4d7e4516a7d954580e2.png\" />, and <img class=\"mwe-math-fallback-png-inline tex\" alt=\" i \in \{ 1,\dots,M \} \" src=\"//upload.wikimedia.org/math/a/c/f/acfa7d3fa8b5dede007f1d23ff9ac34d.png\" /></p>"
"<dl>"
"<dd>(a) Choose a topic <img class=\"mwe-math-fallback-png-inline tex\" alt=\"z_{i,j} \,\sim\, \mathrm{Multinomial}(\theta_i). \" src=\"//upload.wikimedia.org/math/0/1/3/013e65ada20c529347d4e25b403b929c.png\" /></dd>"
"</dl>"
"<dl>"
"<dd>(b) Choose a word <img class=\"mwe-math-fallback-png-inline tex\" alt=\"w_{i,j} \,\sim\, \mathrm{Multinomial}( \phi_{z_{i,j}}) \" src=\"//upload.wikimedia.org/math/1/0/2/1029d4b17e0d5e713beb368760e9afb6.png\" />.</dd>"
"</dl>"
"<p>(Note that the <a href=\"/wiki/Multinomial_distribution\" title=\"Multinomial distribution\">Multinomial distribution</a> here refers to the Multinomial with only one trial. It is formally equivalent to the <a href=\"/wiki/Categorical_distribution\" title=\"Categorical distribution\">categorical distribution</a>.)</p>"
"<p>The lengths <img class=\"mwe-math-fallback-png-inline tex\" alt=\"N_i\" src=\"//upload.wikimedia.org/math/b/4/a/b4ad80684ed94b4380f0d33ba54b91da.png\" /> are treated as independent of all the other data generating variables (<img class=\"mwe-math-fallback-png-inline tex\" alt=\"q\" src=\"//upload.wikimedia.org/math/7/6/9/7694f4a66316e53c8cdd9d9954bd611d.png\" /> and <img class=\"mwe-math-fallback-png-inline tex\" alt=\"z\" src=\"//upload.wikimedia.org/math/f/b/a/fbade9e36a3f36d3d676c1b808451dd7.png\" />). The subscript is often dropped, as in the plate diagrams shown here.</p>"
"<h3><span class=\"mw-headline\" id=\"Mathematical_definition\">Mathematical definition</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Latent_Dirichlet_allocation&amp;action=edit&amp;section=3\" title=\"Edit section: Mathematical definition\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>"
"<p>A formal description of smoothed LDA is as follows:</p>"
"<table class=\"wikitable\">"
"<caption>Definition of variables in the model</caption>"
"<tr>"
"<th>Variable</th>"
"<th>Type</th>"
"<th>Meaning</th>"
"</tr>"
"<tr>"
"<td><img class=\"mwe-math-fallback-png-inline tex\" alt=\"K\" src=\"//upload.wikimedia.org/math/a/5/f/a5f3c6a11b03839d46af9fb43c97c188.png\" /></td>"
"<td>integer</td>"
"<td>number of topics (e.g. 50)</td>"
"</tr>"
"<tr>"
"<td><img class=\"mwe-math-fallback-png-inline tex\" alt=\"V\" src=\"//upload.wikimedia.org/math/5/2/0/5206560a306a2e085a437fd258eb57ce.png\" /></td>"
"<td>integer</td>"
"<td>number of words in the vocabulary (e.g. 50,000 or 1,000,000)</td>"
"</tr>"
"<tr>"
"<td><img class=\"mwe-math-fallback-png-inline tex\" alt=\"M\" src=\"//upload.wikimedia.org/math/6/9/6/69691c7bdcc3ce6d5d8a1361f22d04ac.png\" /></td>"
"<td>integer</td>"
"<td>number of documents</td>"
"</tr>"
"<tr>"
"<td><img class=\"mwe-math-fallback-png-inline tex\" alt=\"N_{d=1 \dots M}\" src=\"//upload.wikimedia.org/math/8/9/6/89641e013410160fabb7851254ac5364.png\" /></td>"
"<td>integer</td>"
"<td>number of words in document <i>d</i></td>"
"</tr>"
"<tr>"
"<td><img class=\"mwe-math-fallback-png-inline tex\" alt=\"N\" src=\"//upload.wikimedia.org/math/8/d/9/8d9c307cb7f3c4a32822a51922d1ceaa.png\" /></td>"
"<td>integer</td>"
"<td>total number of words in all documents; sum of all <img class=\"mwe-math-fallback-png-inline tex\" alt=\"N_d\" src=\"//upload.wikimedia.org/math/a/f/9/af997fdffd97e92479ca9f5a04655bf0.png\" /> values, i.e. <img class=\"mwe-math-fallback-png-inline tex\" alt=\"N = \sum_{d=1}^{M} N_d\" src=\"//upload.wikimedia.org/math/b/4/b/b4bf2f65d7ea7941ef952e9aa871f43e.png\" /></td>"
"</tr>"
"<tr>"
"<td><img class=\"mwe-math-fallback-png-inline tex\" alt=\"\alpha_{k=1 \dots K}\" src=\"//upload.wikimedia.org/math/3/6/4/364b1bc78a060c6c5f382dace2e9852f.png\" /></td>"
"<td>positive real</td>"
"<td>prior weight of topic <i>k</i> in a document; usually the same for all topics; normally a number less than 1, e.g. 0.1, to prefer sparse topic distributions, i.e. few topics per document</td>"
"</tr>"
"<tr>"
"<td><img class=\"mwe-math-fallback-png-inline tex\" alt=\"\boldsymbol\alpha\" src=\"//upload.wikimedia.org/math/a/6/1/a61f5ef20418408f31d9719d660cd414.png\" /></td>"
"<td><i>K</i>-dimension vector of positive reals</td>"
"<td>collection of all <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\alpha_k\" src=\"//upload.wikimedia.org/math/a/a/5/aa5a9b61f854dc3d22a7fd6172bf8ca4.png\" /> values, viewed as a single vector</td>"
"</tr>"
"<tr>"
"<td><img class=\"mwe-math-fallback-png-inline tex\" alt=\"\beta_{w=1 \dots V}\" src=\"//upload.wikimedia.org/math/e/7/4/e74517a958cea11e2e6b51d38061444f.png\" /></td>"
"<td>positive real</td>"
"<td>prior weight of word <i>w</i> in a topic; usually the same for all words; normally a number much less than 1, e.g. 0.001, to strongly prefer sparse word distributions, i.e. few words per topic</td>"
"</tr>"
"<tr>"
"<td><img class=\"mwe-math-fallback-png-inline tex\" alt=\"\boldsymbol\beta\" src=\"//upload.wikimedia.org/math/6/b/f/6bf37cfb9e3eeb9f68c0a32699ac20aa.png\" /></td>"
"<td><i>V</i>-dimension vector of positive reals</td>"
"<td>collection of all <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\beta_w\" src=\"//upload.wikimedia.org/math/9/7/d/97d0ad58971036778aac199c88ca0aa3.png\" /> values, viewed as a single vector</td>"
"</tr>"
"<tr>"
"<td><img class=\"mwe-math-fallback-png-inline tex\" alt=\"\phi_{k=1 \dots K,w=1 \dots V}\" src=\"//upload.wikimedia.org/math/c/6/b/c6b2698ac657df2add6c9ed5cb7b963e.png\" /></td>"
"<td>probability (real number between 0 and 1)</td>"
"<td>probability of word <i>w</i> occurring in topic <i>k</i></td>"
"</tr>"
"<tr>"
"<td><img class=\"mwe-math-fallback-png-inline tex\" alt=\"\boldsymbol\phi_{k=1 \dots K}\" src=\"//upload.wikimedia.org/math/7/a/d/7adaa8355c3b7f2e4a65fafc4ba8d31c.png\" /></td>"
"<td><i>V</i>-dimension vector of probabilities, which must sum to 1</td>"
"<td>distribution of words in topic <i>k</i></td>"
"</tr>"
"<tr>"
"<td><img class=\"mwe-math-fallback-png-inline tex\" alt=\"\theta_{d=1 \dots M,k=1 \dots K}\" src=\"//upload.wikimedia.org/math/a/8/0/a80414d68c4a23e6b710b56a0bdfc7d8.png\" /></td>"
"<td>probability (real number between 0 and 1)</td>"
"<td>probability of topic <i>k</i> occurring in document <i>d</i> for any given word</td>"
"</tr>"
"<tr>"
"<td><img class=\"mwe-math-fallback-png-inline tex\" alt=\"\boldsymbol\theta_{d=1 \dots M}\" src=\"//upload.wikimedia.org/math/3/a/8/3a8b8ccab17057152d73dc1bd5d26f06.png\" /></td>"
"<td><i>K</i>-dimension vector of probabilities, which must sum to 1</td>"
"<td>distribution of topics in document <i>d</i></td>"
"</tr>"
"<tr>"
"<td><img class=\"mwe-math-fallback-png-inline tex\" alt=\"z_{d=1 \dots M,w=1 \dots N_d}\" src=\"//upload.wikimedia.org/math/c/e/2/ce226e7add76187e9938d2de726542a7.png\" /></td>"
"<td>integer between 1 and <i>K</i></td>"
"<td>identity of topic of word <i>w</i> in document <i>d</i></td>"
"</tr>"
"<tr>"
"<td><img class=\"mwe-math-fallback-png-inline tex\" alt=\"\mathbf{Z}\" src=\"//upload.wikimedia.org/math/8/0/4/804f17c41b884c2c7084fecd4edd2134.png\" /></td>"
"<td><i>N</i>-dimension vector of integers between 1 and <i>K</i></td>"
"<td>identity of topic of all words in all documents</td>"
"</tr>"
"<tr>"
"<td><img class=\"mwe-math-fallback-png-inline tex\" alt=\"w_{d=1 \dots M,w=1 \dots N_d}\" src=\"//upload.wikimedia.org/math/a/8/9/a89b892bfd117f5cbcebdf245b5871cf.png\" /></td>"
"<td>integer between 1 and <i>V</i></td>"
"<td>identity of word <i>w</i> in document <i>d</i></td>"
"</tr>"
"<tr>"
"<td><img class=\"mwe-math-fallback-png-inline tex\" alt=\"\mathbf{W}\" src=\"//upload.wikimedia.org/math/1/9/9/1990b12fd270f7d985212dc61cf337af.png\" /></td>"
"<td><i>N</i>-dimension vector of integers between 1 and <i>V</i></td>"
"<td>identity of all words in all documents</td>"
"</tr>"
"</table>"
"<p>We can then mathematically describe the random variables as follows:</p>"
"<dl>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\""
"\begin{array}{lcl}"
"\boldsymbol\phi_{k=1 \dots K} &amp;\sim&amp; \operatorname{Dirichlet}_V(\boldsymbol\beta) \\"
"\boldsymbol\theta_{d=1 \dots M} &amp;\sim&amp; \operatorname{Dirichlet}_K(\boldsymbol\alpha) \\"
"z_{d=1 \dots M,w=1 \dots N_d} &amp;\sim&amp; \operatorname{Categorical}_K(\boldsymbol\theta_d) \\"
"w_{d=1 \dots M,w=1 \dots N_d} &amp;\sim&amp; \operatorname{Categorical}_V(\boldsymbol\phi_{z_{dw}}) \\"
"\end{array}"
"\" src=\"//upload.wikimedia.org/math/e/1/1/e11e2ba2f160f3f0452b37b76441b95a.png\" /></dd>"
"</dl>"
"<h2><span class=\"mw-headline\" id=\"Inference\">Inference</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Latent_Dirichlet_allocation&amp;action=edit&amp;section=4\" title=\"Edit section: Inference\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>"
"<div class=\"hatnote boilerplate seealso\">See also: <a href=\"/wiki/Dirichlet-multinomial_distribution\" title=\"Dirichlet-multinomial distribution\">Dirichlet-multinomial distribution</a></div>"
"<p>Learning the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) is a problem of <a href=\"/wiki/Bayesian_inference\" title=\"Bayesian inference\">Bayesian inference</a>. The original paper used a <a href=\"/wiki/Variational_Bayes\" title=\"Variational Bayes\" class=\"mw-redirect\">variational Bayes</a> approximation of the <a href=\"/wiki/Posterior_distribution\" title=\"Posterior distribution\" class=\"mw-redirect\">posterior distribution</a>;<sup id=\"cite_ref-blei2003_1-1\" class=\"reference\"><a href=\"#cite_note-blei2003-1\"><span>[</span>1<span>]</span></a></sup> alternative inference techniques use <a href=\"/wiki/Gibbs_sampling\" title=\"Gibbs sampling\">Gibbs sampling</a><sup id=\"cite_ref-3\" class=\"reference\"><a href=\"#cite_note-3\"><span>[</span>3<span>]</span></a></sup> and <a href=\"/wiki/Expectation_propagation\" title=\"Expectation propagation\">expectation propagation</a>.<sup id=\"cite_ref-4\" class=\"reference\"><a href=\"#cite_note-4\"><span>[</span>4<span>]</span></a></sup></p>"
"<p>Following is the derivation of the equations for <a href=\"/wiki/Collapsed_Gibbs_sampling\" title=\"Collapsed Gibbs sampling\" class=\"mw-redirect\">collapsed Gibbs sampling</a>, which means <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\varphi\" src=\"//upload.wikimedia.org/math/3/5/3/3538eb9c84efdcbd130c4c953781cfdb.png\" />s and <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\theta\" src=\"//upload.wikimedia.org/math/5/0/d/50d91f80cbb8feda1d10e167107ad1ff.png\" />s will be integrated out. For simplicity, in this derivation the documents are all assumed to have the same length <img class=\"mwe-math-fallback-png-inline tex\" alt=\"N_{}\" src=\"//upload.wikimedia.org/math/2/1/0/210eab8f974251f76c5210397d83f746.png\" />. The derivation is equally valid if the document lengths vary.</p>"
"<p>According to the model, the total probability of the model is:</p>"
"<dl>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\" P(\boldsymbol{W}, \boldsymbol{Z}, \boldsymbol{\theta},"
"\boldsymbol{\varphi};\alpha,\beta) = \prod_{i=1}^K"
"P(\varphi_i;\beta) \prod_{j=1}^M P(\theta_j;\alpha) \prod_{t=1}^N"
"P(Z_{j,t}|\theta_j)P(W_{j,t}|\varphi_{Z_{j,t}}) ,\" src=\"//upload.wikimedia.org/math/c/5/4/c5421420d42daecde942b9e42472a7cb.png\" /></dd>"
"</dl>"
"<p>where the bold-font variables denote the vector version of the variables. First of all, <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\boldsymbol{\varphi}\" src=\"//upload.wikimedia.org/math/b/2/1/b21e3725cad5fa80e8b53c07ec4b0ee3.png\" /> and <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\boldsymbol{\theta}\" src=\"//upload.wikimedia.org/math/2/8/9/28964e5d44aaa561601a17e49bb5a8d9.png\" /> need to be integrated out.</p>"
"<dl>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\""
"\begin{align}"
"&amp;P(\boldsymbol{Z}, \boldsymbol{W};\alpha,\beta)  =  \int_{\boldsymbol{\theta}} \int_{\boldsymbol{\varphi}} P(\boldsymbol{W}, \boldsymbol{Z}, \boldsymbol{\theta}, \boldsymbol{\varphi};\alpha,\beta) \, d\boldsymbol{\varphi} \, d\boldsymbol{\theta} \\"
" = &amp; \int_{\boldsymbol{\varphi}} \prod_{i=1}^K P(\varphi_i;\beta) \prod_{j=1}^M \prod_{t=1}^N P(W_{j,t}|\varphi_{Z_{j,t}}) \, d\boldsymbol{\varphi} \int_{\boldsymbol{\theta}} \prod_{j=1}^M P(\theta_j;\alpha) \prod_{t=1}^N P(Z_{j,t}|\theta_j) \, d\boldsymbol{\theta}."
"\end{align}"
"\" src=\"//upload.wikimedia.org/math/c/4/f/c4f47250b883016c6630af3697bad4cc.png\" /></dd>"
"</dl>"
"<p>All the <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\theta\" src=\"//upload.wikimedia.org/math/5/0/d/50d91f80cbb8feda1d10e167107ad1ff.png\" />s are independent to each other and the same to all the <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\varphi\" src=\"//upload.wikimedia.org/math/3/5/3/3538eb9c84efdcbd130c4c953781cfdb.png\" />s. So we can treat each <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\theta\" src=\"//upload.wikimedia.org/math/5/0/d/50d91f80cbb8feda1d10e167107ad1ff.png\" /> and each <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\varphi\" src=\"//upload.wikimedia.org/math/3/5/3/3538eb9c84efdcbd130c4c953781cfdb.png\" /> separately. We now focus only on the <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\theta\" src=\"//upload.wikimedia.org/math/5/0/d/50d91f80cbb8feda1d10e167107ad1ff.png\" /> part.</p>"
"<dl>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\""
"\int_{\boldsymbol{\theta}} \prod_{j=1}^M P(\theta_j;\alpha) \prod_{t=1}^N P(Z_{j,t}|\theta_j) d\boldsymbol{\theta} = \prod_{j=1}^M \int_{\theta_j} P(\theta_j;\alpha) \prod_{t=1}^N"
"P(Z_{j,t}|\theta_j) \, d\theta_j ."
"\" src=\"//upload.wikimedia.org/math/f/c/7/fc70029c3a73a65dada4c1b73a53afda.png\" /></dd>"
"</dl>"
"<p>We can further focus on only one <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\theta\" src=\"//upload.wikimedia.org/math/5/0/d/50d91f80cbb8feda1d10e167107ad1ff.png\" /> as the following:</p>"
"<dl>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\" \int_{\theta_j} P(\theta_j;\alpha) \prod_{t=1}^N"
"P(Z_{j,t}|\theta_j) \, d\theta_j .\" src=\"//upload.wikimedia.org/math/5/b/3/5b363a4855c6074cbb037d7d37772e02.png\" /></dd>"
"</dl>"
"<p>Actually, it is the hidden part of the model for the <img class=\"mwe-math-fallback-png-inline tex\" alt=\"j^{th}\" src=\"//upload.wikimedia.org/math/b/3/5/b35be813c0c020ea8a099c67d611e463.png\" /> document. Now we replace the probabilities in the above equation by the true distribution expression to write out the explicit equation.</p>"
"<dl>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\""
"\begin{align}"
"&amp; \int_{\theta_j} P(\theta_j;\alpha) \prod_{t=1}^N P(Z_{j,t}|\theta_j) \, d\theta_j  "
" = &amp; \int_{\theta_j} \frac{\Gamma\bigl(\sum_{i=1}^K \alpha_i \bigr)}{\prod_{i=1}^K \Gamma(\alpha_i)} \prod_{i=1}^K \theta_{j,i}^{\alpha_i - 1} \prod_{t=1}^N P(Z_{j,t}|\theta_j) \, d\theta_j."
"\end{align}"
"\" src=\"//upload.wikimedia.org/math/1/7/5/17541563f2162eba129d911c3e0154fc.png\" /></dd>"
"</dl>"
"<p>Let <img class=\"mwe-math-fallback-png-inline tex\" alt=\"n_{j,r}^i\" src=\"//upload.wikimedia.org/math/d/8/3/d833b4a058ef5a58aca9b91f963ce1e7.png\" /> be the number of word tokens in the <img class=\"mwe-math-fallback-png-inline tex\" alt=\"j^{th}\" src=\"//upload.wikimedia.org/math/b/3/5/b35be813c0c020ea8a099c67d611e463.png\" /> document with the same word symbol (the <img class=\"mwe-math-fallback-png-inline tex\" alt=\"r^{th}\" src=\"//upload.wikimedia.org/math/4/2/3/4236cc5bacb811646eade06b1b688638.png\" /> word in the vocabulary) assigned to the <img class=\"mwe-math-fallback-png-inline tex\" alt=\"i^{th}\" src=\"//upload.wikimedia.org/math/2/1/4/21479f1e549e1fcbde3442f4a1a4db21.png\" /> topic. So, <img class=\"mwe-math-fallback-png-inline tex\" alt=\"n_{j,r}^i\" src=\"//upload.wikimedia.org/math/d/8/3/d833b4a058ef5a58aca9b91f963ce1e7.png\" /> is three dimensional. If any of the three dimensions is not limited to a specific value, we use a parenthesized point <img class=\"mwe-math-fallback-png-inline tex\" alt=\"(\cdot)\" src=\"//upload.wikimedia.org/math/f/3/4/f34dd6e523b75fa8408b81c9a95e84fb.png\" /> to denote. For example, <img class=\"mwe-math-fallback-png-inline tex\" alt=\"n_{j,(\cdot)}^i\" src=\"//upload.wikimedia.org/math/a/4/0/a40c68c4bc6ce7160ae577cef4da0a99.png\" /> denotes the number of word tokens in the <img class=\"mwe-math-fallback-png-inline tex\" alt=\"j^{th}\" src=\"//upload.wikimedia.org/math/b/3/5/b35be813c0c020ea8a099c67d611e463.png\" /> document assigned to the <img class=\"mwe-math-fallback-png-inline tex\" alt=\"i^{th}\" src=\"//upload.wikimedia.org/math/2/1/4/21479f1e549e1fcbde3442f4a1a4db21.png\" /> topic. Thus, the right most part of the above equation can be rewritten as:</p>"
"<dl>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\" \prod_{t=1}^N P(Z_{j,t}|\theta_j)  =  \prod_{i=1}^K"
"\theta_{j,i}^{n_{j,(\cdot)}^i} .\" src=\"//upload.wikimedia.org/math/1/8/c/18ca693eb10b219b5468b01de4f8d644.png\" /></dd>"
"</dl>"
"<p>So the <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\theta_j\" src=\"//upload.wikimedia.org/math/b/b/b/bbbc2fd79dc282edae4c9597be881176.png\" /> integration formula can be changed to:</p>"
"<dl>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\""
"\begin{align}"
"&amp; \int_{\theta_j} \frac{\Gamma\bigl(\sum_{i=1}^K \alpha_i \bigr)}{\prod_{i=1}^K \Gamma(\alpha_i)} \prod_{i=1}^K \theta_{j,i}^{\alpha_i - 1} \prod_{i=1}^K \theta_{j,i}^{n_{j,(\cdot)}^i} \, d\theta_j \\"
" = &amp; \int_{\theta_j} \frac{\Gamma\bigl(\sum_{i=1}^K \alpha_i \bigr)}{\prod_{i=1}^K \Gamma(\alpha_i)} \prod_{i=1}^K \theta_{j,i}^{n_{j,(\cdot)}^i+\alpha_i - 1} \, d\theta_j."
"\end{align}"
"\" src=\"//upload.wikimedia.org/math/6/a/c/6acfbcc6aff643eeea3c6371b05d1593.png\" /></dd>"
"</dl>"
"<p>Clearly, the equation inside the integration has the same form as the <a href=\"/wiki/Dirichlet_distribution\" title=\"Dirichlet distribution\">Dirichlet distribution</a>. According to the <a href=\"/wiki/Dirichlet_distribution\" title=\"Dirichlet distribution\">Dirichlet distribution</a>,</p>"
"<dl>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\" \int_{\theta_j} \frac{\Gamma\bigl(\sum_{i=1}^K"
"n_{j,(\cdot)}^i+\alpha_i \bigr)}{\prod_{i=1}^K"
"\Gamma(n_{j,(\cdot)}^i+\alpha_i)} \prod_{i=1}^K"
"\theta_{j,i}^{n_{j,(\cdot)}^i+\alpha_i - 1} \, d\theta_j =1 .\" src=\"//upload.wikimedia.org/math/b/2/f/b2f7fd21fccd535c335230c021c66073.png\" /></dd>"
"</dl>"
"<p>Thus,</p>"
"<dl>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\""
"\begin{align}"
"&amp; \int_{\theta_j} P(\theta_j;\alpha) \prod_{t=1}^N P(Z_{j,t}|\theta_j) \, d\theta_j = \int_{\theta_j} \frac{\Gamma\bigl(\sum_{i=1}^K \alpha_i \bigr)}{\prod_{i=1}^K \Gamma(\alpha_i)} \prod_{i=1}^K \theta_{j,i}^{n_{j,(\cdot)}^i+\alpha_i - 1} \, d\theta_j \\"
"= &amp; \frac{\Gamma\bigl(\sum_{i=1}^K \alpha_i \bigr)}{\prod_{i=1}^K \Gamma(\alpha_i)}\frac{\prod_{i=1}^K \Gamma(n_{j,(\cdot)}^i+\alpha_i)}{\Gamma\bigl(\sum_{i=1}^K n_{j,(\cdot)}^i+\alpha_i \bigr)} \int_{\theta_j} \frac{\Gamma\bigl(\sum_{i=1}^K n_{j,(\cdot)}^i+\alpha_i \bigr)}{\prod_{i=1}^K \Gamma(n_{j,(\cdot)}^i+\alpha_i)} \prod_{i=1}^K \theta_{j,i}^{n_{j,(\cdot)}^i+\alpha_i - 1} \, d\theta_j \\"
"= &amp; \frac{\Gamma\bigl(\sum_{i=1}^K \alpha_i \bigr)}{\prod_{i=1}^K \Gamma(\alpha_i)}\frac{\prod_{i=1}^K \Gamma(n_{j,(\cdot)}^i+\alpha_i)}{\Gamma\bigl(\sum_{i=1}^K n_{j,(\cdot)}^i+\alpha_i \bigr)}."
"\end{align}"
"\" src=\"//upload.wikimedia.org/math/b/6/5/b658fcc850a54839f3ed99b88135cb94.png\" /></dd>"
"</dl>"
"<p>Now we turn our attentions to the <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\boldsymbol{\varphi}\" src=\"//upload.wikimedia.org/math/b/2/1/b21e3725cad5fa80e8b53c07ec4b0ee3.png\" /> part. Actually, the derivation of the <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\boldsymbol{\varphi}\" src=\"//upload.wikimedia.org/math/b/2/1/b21e3725cad5fa80e8b53c07ec4b0ee3.png\" /> part is very similar to the <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\boldsymbol{\theta}\" src=\"//upload.wikimedia.org/math/2/8/9/28964e5d44aaa561601a17e49bb5a8d9.png\" /> part. Here we only list the steps of the derivation:</p>"
"<dl>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\""
"\begin{align}"
"&amp; \int_{\boldsymbol{\varphi}} \prod_{i=1}^K P(\varphi_i;\beta) \prod_{j=1}^M \prod_{t=1}^N P(W_{j,t}|\varphi_{Z_{j,t}}) \, d\boldsymbol{\varphi} \\"
"= &amp; \prod_{i=1}^K \int_{\varphi_i} P(\varphi_i;\beta) \prod_{j=1}^M \prod_{t=1}^N P(W_{j,t}|\varphi_{Z_{j,t}}) \, d\varphi_i\\"
"= &amp; \prod_{i=1}^K \int_{\varphi_i} \frac{\Gamma\bigl(\sum_{r=1}^V \beta_r \bigr)}{\prod_{r=1}^V \Gamma(\beta_r)} \prod_{r=1}^V \varphi_{i,r}^{\beta_r - 1}  \prod_{r=1}^V \varphi_{i,r}^{n_{(\cdot),r}^i} \, d\varphi_i \\"
"= &amp; \prod_{i=1}^K \int_{\varphi_i} \frac{\Gamma\bigl(\sum_{r=1}^V \beta_r \bigr)}{\prod_{r=1}^V \Gamma(\beta_r)} \prod_{r=1}^V \varphi_{i,r}^{n_{(\cdot),r}^i+\beta_r - 1} \, d\varphi_i \\"
"= &amp; \prod_{i=1}^K \frac{\Gamma\bigl(\sum_{r=1}^V \beta_r"
"\bigr)}{\prod_{r=1}^V \Gamma(\beta_r)}\frac{\prod_{r=1}^V"
"\Gamma(n_{(\cdot),r}^i+\beta_r)}{\Gamma\bigl(\sum_{r=1}^V"
"n_{(\cdot),r}^i+\beta_r \bigr)} ."
"\end{align}"
"\" src=\"//upload.wikimedia.org/math/9/6/3/9635f21c788df7c9903d2464bbe1947f.png\" /></dd>"
"</dl>"
"<p>For clarity, here we write down the final equation with both <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\boldsymbol{\phi}\" src=\"//upload.wikimedia.org/math/5/4/9/5493fdcb1b87c04723db9894653f3ae5.png\" /> and <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\boldsymbol{\theta}\" src=\"//upload.wikimedia.org/math/2/8/9/28964e5d44aaa561601a17e49bb5a8d9.png\" /> integrated out:</p>"
"<dl>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\""
"\begin{align}"
"&amp; P(\boldsymbol{Z}, \boldsymbol{W};\alpha,\beta) \\"
"= &amp; \prod_{j=1}^M  \frac{\Gamma\bigl(\sum_{i=1}^K \alpha_i"
"\bigr)}{\prod_{i=1}^K \Gamma(\alpha_i)}\frac{\prod_{i=1}^K"
"\Gamma(n_{j,(\cdot)}^i+\alpha_i)}{\Gamma\bigl(\sum_{i=1}^K"
"n_{j,(\cdot)}^i+\alpha_i \bigr)} \times \prod_{i=1}^K"
"\frac{\Gamma\bigl(\sum_{r=1}^V \beta_r \bigr)}{\prod_{r=1}^V"
"\Gamma(\beta_r)}\frac{\prod_{r=1}^V"
"\Gamma(n_{(\cdot),r}^i+\beta_r)}{\Gamma\bigl(\sum_{r=1}^V"
"n_{(\cdot),r}^i+\beta_r \bigr)} . "
"\end{align}"
"\" src=\"//upload.wikimedia.org/math/0/c/8/0c8ce6e5dece9b71e08ce042e4b0cdbb.png\" /></dd>"
"</dl>"
"<p>The goal of Gibbs Sampling here is to approximate the distribution of <img class=\"mwe-math-fallback-png-inline tex\" alt=\"P(\boldsymbol{Z}|\boldsymbol{W};\alpha,\beta)\" src=\"//upload.wikimedia.org/math/1/0/0/100c6ff5c05a1e881baf954756f22426.png\" />. Since <img class=\"mwe-math-fallback-png-inline tex\" alt=\"P(\boldsymbol{W};\alpha,\beta)\" src=\"//upload.wikimedia.org/math/4/8/f/48f40306a485b7271f20b9fbbb103feb.png\" /> is invariable for any of Z, Gibbs Sampling equations can be derived from <img class=\"mwe-math-fallback-png-inline tex\" alt=\"P(\boldsymbol{Z}, \boldsymbol{W};\alpha,\beta)\" src=\"//upload.wikimedia.org/math/7/5/9/759646a53b7c4c6092bfcb45a8e16a6a.png\" /> directly. The key point is to derive the following conditional probability:</p>"
"<dl>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\" P(Z_{(m,n)}|\boldsymbol{Z_{-(m,n)}},"
"\boldsymbol{W};\alpha,\beta)=\frac{P(Z_{(m,n)},"
"\boldsymbol{Z_{-(m,n)}},\boldsymbol{W};\alpha,\beta)}"
"{P(\boldsymbol{Z_{-(m,n)}}, \boldsymbol{W};\alpha,\beta)} ,\" src=\"//upload.wikimedia.org/math/b/1/1/b11c288a5152deeae5e5a4457caea30e.png\" /></dd>"
"</dl>"
"<p>where <img class=\"mwe-math-fallback-png-inline tex\" alt=\"Z_{(m,n)}\" src=\"//upload.wikimedia.org/math/8/0/0/800801dcf973ce6b438670f681b0d250.png\" /> denotes the <img class=\"mwe-math-fallback-png-inline tex\" alt=\"Z\" src=\"//upload.wikimedia.org/math/2/1/c/21c2e59531c8710156d34a3c30ac81d5.png\" /> hidden variable of the <img class=\"mwe-math-fallback-png-inline tex\" alt=\"n^{th}\" src=\"//upload.wikimedia.org/math/c/c/7/cc778408df16bd74114dcbb47797a740.png\" /> word token in the <img class=\"mwe-math-fallback-png-inline tex\" alt=\"m^{th}\" src=\"//upload.wikimedia.org/math/7/3/6/736b3fb7cb6161514cdb457977eb8e7f.png\" /> document. And further we assume that the word symbol of it is the <img class=\"mwe-math-fallback-png-inline tex\" alt=\"v^{th}\" src=\"//upload.wikimedia.org/math/0/5/7/05767281191667bfe3e0f4802eb8016e.png\" /> word in the vocabulary. <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\boldsymbol{Z_{-(m,n)}}\" src=\"//upload.wikimedia.org/math/c/2/a/c2a6a2993ab3be4bd90ddb311bc31b16.png\" /> denotes all the <img class=\"mwe-math-fallback-png-inline tex\" alt=\"Z\" src=\"//upload.wikimedia.org/math/2/1/c/21c2e59531c8710156d34a3c30ac81d5.png\" />s but <img class=\"mwe-math-fallback-png-inline tex\" alt=\"Z_{(m,n)}\" src=\"//upload.wikimedia.org/math/8/0/0/800801dcf973ce6b438670f681b0d250.png\" />. Note that Gibbs Sampling needs only to sample a value for <img class=\"mwe-math-fallback-png-inline tex\" alt=\"Z_{(m,n)}\" src=\"//upload.wikimedia.org/math/8/0/0/800801dcf973ce6b438670f681b0d250.png\" />, according to the above probability, we do not need the exact value of <img class=\"mwe-math-fallback-png-inline tex\" alt=\"P(Z_{m,n}|\boldsymbol{Z_{-(m,n)}},"
"\boldsymbol{W};\alpha,\beta)\" src=\"//upload.wikimedia.org/math/7/d/1/7d13ec7914963ed77bb74d240c66e3de.png\" /> but the ratios among the probabilities that <img class=\"mwe-math-fallback-png-inline tex\" alt=\"Z_{(m,n)}\" src=\"//upload.wikimedia.org/math/8/0/0/800801dcf973ce6b438670f681b0d250.png\" /> can take value. So, the above equation can be simplified as:</p>"
"<dl>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\""
"\begin{align}"
"&amp; P(Z_{(m,n)}=k|\boldsymbol{Z_{-(m,n)}}, \boldsymbol{W};\alpha,\beta) \\"
"\propto &amp;"
"P(Z_{(m,n)}=k,\boldsymbol{Z_{-(m,n)}},\boldsymbol{W};\alpha,\beta) \\"
"= &amp; \left(\frac{\Gamma\left(\sum_{i=1}^K \alpha_i"
"\right)}{\prod_{i=1}^K \Gamma(\alpha_i)}\right)^M \prod_{j\neq m}"
"\frac{\prod_{i=1}^K"
"\Gamma(n_{j,(\cdot)}^i+\alpha_i)}{\Gamma\bigl(\sum_{i=1}^K"
"n_{j,(\cdot)}^i+\alpha_i \bigr)} \\"
"&amp; \times \left( \frac{\Gamma\bigl(\sum_{r=1}^V \beta_r"
"\bigr)}{\prod_{r=1}^V \Gamma(\beta_r)}\right)^K \prod_{i=1}^K"
"\prod_{r\neq v}"
"\Gamma(n_{(\cdot),r}^i+\beta_r) \\"
"&amp; \times  \frac{\prod_{i=1}^K"
"\Gamma(n_{m,(\cdot)}^i+\alpha_i)}{\Gamma\bigl(\sum_{i=1}^K"
"n_{m,(\cdot)}^i+\alpha_i \bigr)}  \prod_{i=1}^K \frac{"
"\Gamma(n_{(\cdot),v}^i+\beta_v)}{\Gamma\bigl(\sum_{r=1}^V"
"n_{(\cdot),r}^i+\beta_r \bigr)} \\"
"\propto &amp; \frac{\prod_{i=1}^K"
"\Gamma(n_{m,(\cdot)}^i+\alpha_i)}{\Gamma\bigl(\sum_{i=1}^K"
"n_{m,(\cdot)}^i+\alpha_i \bigr)}  \prod_{i=1}^K \frac{"
"\Gamma(n_{(\cdot),v}^i+\beta_v)}{\Gamma\bigl(\sum_{r=1}^V"
"n_{(\cdot),r}^i+\beta_r \bigr)}."
"\end{align}"
"\" src=\"//upload.wikimedia.org/math/1/e/9/1e90cc19d4044f0f21d2687bc424489c.png\" /></dd>"
"</dl>"
"<p>Finally, let <img class=\"mwe-math-fallback-png-inline tex\" alt=\"n_{j,r}^{i,-(m,n)}\" src=\"//upload.wikimedia.org/math/c/8/7/c870211e894d14ee49a565f70b873f28.png\" /> be the same meaning as <img class=\"mwe-math-fallback-png-inline tex\" alt=\"n_{j,r}^i\" src=\"//upload.wikimedia.org/math/d/8/3/d833b4a058ef5a58aca9b91f963ce1e7.png\" /> but with the <img class=\"mwe-math-fallback-png-inline tex\" alt=\"Z_{(m,n)}\" src=\"//upload.wikimedia.org/math/8/0/0/800801dcf973ce6b438670f681b0d250.png\" /> excluded. The above equation can be further simplified by treating terms not dependent on <img class=\"mwe-math-fallback-png-inline tex\" alt=\"k\" src=\"//upload.wikimedia.org/math/8/c/e/8ce4b16b22b58894aa86c421e8759df3.png\" /> as constants:</p>"
"<dl>"
"<dd><img class=\"mwe-math-fallback-png-inline tex\" alt=\""
"\begin{align}"
"\propto &amp; \frac{\prod_{i\neq k}"
"\Gamma(n_{m,(\cdot)}^{i,-(m,n)}+\alpha_i)}{\Gamma\bigl((\sum_{i=1}^K"
"n_{m,(\cdot)}^{i,-(m,n)}+\alpha_i) +1\bigr)}  \prod_{i\neq k} \frac{"
"\Gamma(n_{(\cdot),v}^{i,-(m,n)}+\beta_v)}{\Gamma\bigl(\sum_{r=1}^V"
"n_{(\cdot),r}^{i,-(m,n)}+\beta_r \bigr)}\\"
"\times &amp; \Gamma(n_{m,(\cdot)}^{k,-(m,n)}+\alpha_k + 1) \frac{"
"\Gamma(n_{(\cdot),v}^{k,-(m,n)}+\beta_v +"
"1)}{\Gamma\bigl((\sum_{r=1}^V n_{(\cdot),r}^{k,-(m,n)}+\beta_r)+1"
"\bigr)} \\"
"\propto &amp; \frac{\Gamma(n_{m,(\cdot)}^{k,-(m,n)}+\alpha_k +"
"1)}{\Gamma\bigl((\sum_{i=1}^K n_{m,(\cdot)}^{i,-(m,n)}+\alpha_i)+1"
"\bigr)} \frac{ \Gamma(n_{(\cdot),v}^{k,-(m,n)}+\beta_v +"
"1)}{\Gamma\bigl((\sum_{r=1}^V n_{(\cdot),r}^{k,-(m,n)}+\beta_r)+1"
"\bigr)}\\"
"= &amp;"
"\frac{\Gamma(n_{m,(\cdot)}^{k,-(m,n)}+\alpha_k)\bigl(n_{m,(\cdot)}^{k,-(m,n)}+\alpha_k\bigr)}"
"{\Gamma\bigl(\sum_{i=1}^K"
"n_{m,(\cdot)}^{i,-(m,n)}+\alpha_i\bigr)\bigl(\sum_{i=1}^K"
"n_{m,(\cdot)}^{i,-(m,n)}+\alpha_i\bigr)} \frac{"
"\Gamma\bigl(n_{(\cdot),v}^{k,-(m,n)}+\beta_v\bigr)\bigl(n_{(\cdot),v}^{k,-(m,n)}+\beta_v\bigr)}"
"{\Gamma\bigl(\sum_{r=1}^V n_{(\cdot),r}^{k,-(m,n)}+\beta_r\bigr)"
"\bigl(\sum_{r=1}^V n_{(\cdot),r}^{k,-(m,n)}+\beta_r)} \\"
"\propto &amp; \frac{\bigl(n_{m,(\cdot)}^{k,-(m,n)}+\alpha_k\bigr)}"
"{\bigl(\sum_{i=1}^K n_{m,(\cdot)}^{i,-(m,n)}+\alpha_i\bigr)} \frac{"
"\bigl(n_{(\cdot),v}^{k,-(m,n)}+\beta_v\bigr)} {\bigl(\sum_{r=1}^V"
"n_{(\cdot),r}^{k,-(m,n)}+\beta_r)}\\"
"\propto &amp; \bigl(n_{m,(\cdot)}^{k,-(m,n)}+\alpha_k\bigr)\frac{"
"\bigl(n_{(\cdot),v}^{k,-(m,n)}+\beta_v\bigr)} {\bigl(\sum_{r=1}^V"
"n_{(\cdot),r}^{k,-(m,n)}+\beta_r)}."
"\end{align}"
"\" src=\"//upload.wikimedia.org/math/9/5/3/9537d8b9110c42f4e80354ccfde4fb06.png\" /></dd>"
"</dl>"
"<p>Note that the same formula is derived in the article on the <a href=\"/wiki/Dirichlet-multinomial_distribution#A_combined_example:_LDA_topic_models\" title=\"Dirichlet-multinomial distribution\">Dirichlet-multinomial distribution</a>, as part of a more general discussion of integrating <a href=\"/wiki/Dirichlet_distribution\" title=\"Dirichlet distribution\">Dirichlet distribution</a> priors out of a <a href=\"/wiki/Bayesian_network\" title=\"Bayesian network\">Bayesian network</a>.</p>"
"<h2><span class=\"mw-headline\" id=\"Applications.2C_extensions_and_similar_techniques\">Applications, extensions and similar techniques</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Latent_Dirichlet_allocation&amp;action=edit&amp;section=5\" title=\"Edit section: Applications, extensions and similar techniques\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>"
"<p>Topic modeling is a classic problem in <a href=\"/wiki/Information_retrieval\" title=\"Information retrieval\">information retrieval</a>. Related models and techniques are, among others, <a href=\"/wiki/Latent_semantic_indexing\" title=\"Latent semantic indexing\">latent semantic indexing</a>, <a href=\"/wiki/Independent_component_analysis\" title=\"Independent component analysis\">independent component analysis</a>, <a href=\"/wiki/PLSI\" title=\"PLSI\" class=\"mw-redirect\">probabilistic latent semantic indexing</a>, <a href=\"/wiki/Non-negative_matrix_factorization\" title=\"Non-negative matrix factorization\">non-negative matrix factorization</a>, and <a href=\"/wiki/Gamma-Poisson_distribution\" title=\"Gamma-Poisson distribution\" class=\"mw-redirect\">Gamma-Poisson distribution</a>.</p>"
"<p>The LDA model is highly modular and can therefore be easily extended. The main field of interest is modeling relations between topics. This is achieved by using another distribution on the simplex instead of the Dirichlet. The Correlated Topic Model<sup id=\"cite_ref-5\" class=\"reference\"><a href=\"#cite_note-5\"><span>[</span>5<span>]</span></a></sup> follows this approach, inducing a correlation structure between topics by using the <a href=\"/wiki/Logistic_normal_distribution\" title=\"Logistic normal distribution\" class=\"mw-redirect\">logistic normal distribution</a> instead of the Dirichlet. Another extension is the hierarchical LDA (hLDA),<sup id=\"cite_ref-6\" class=\"reference\"><a href=\"#cite_note-6\"><span>[</span>6<span>]</span></a></sup> where topics are joined together in a hierarchy by using the nested <a href=\"/wiki/Chinese_restaurant_process\" title=\"Chinese restaurant process\">Chinese restaurant process</a>. LDA can also be extended to a corpus in which a document includes two types of information (e.g., words and names), as in the <a href=\"/w/index.php?title=LDA-dual_model&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"LDA-dual model (page does not exist)\">LDA-dual model</a>.<sup id=\"cite_ref-7\" class=\"reference\"><a href=\"#cite_note-7\"><span>[</span>7<span>]</span></a></sup> Nonparametric extensions of LDA include the <a href=\"/wiki/Hierarchical_Dirichlet_process\" title=\"Hierarchical Dirichlet process\">Hierarchical Dirichlet process</a> mixture model, which allows the number of topics to be unbounded and learnt from data and the <a href=\"/w/index.php?title=Nested_Chinese_Restaurant_Process&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Nested Chinese Restaurant Process (page does not exist)\">Nested Chinese Restaurant Process</a> which allows topics to be arranged in a hierarchy whose structure is learnt from data.</p>"
"<p>As noted earlier, PLSA is similar to LDA. The LDA model is essentially the Bayesian version of PLSA model. The Bayesian formulation tends to perform better on small datasets because Bayesian methods can avoid overfitting the data. In a very large dataset, the results are probably the same. One difference is that PLSA uses a variable <img class=\"mwe-math-fallback-png-inline tex\" alt=\"d\" src=\"//upload.wikimedia.org/math/8/2/7/8277e0910d750195b448797616e091ad.png\" /> to represent a document in the training set. So in PLSA, when presented with a document the model hasn't seen before, we fix <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\Pr(w \mid z)\" src=\"//upload.wikimedia.org/math/3/7/8/378ec7adbc08b8d529f92de4154d4a19.png\" />—the probability of words under topics—to be that learned from the training set and use the same EM algorithm to infer <img class=\"mwe-math-fallback-png-inline tex\" alt=\"\Pr(z \mid d)\" src=\"//upload.wikimedia.org/math/d/4/1/d418ac8dd2c8289994647268c30f7a5f.png\" />—the topic distribution under <img class=\"mwe-math-fallback-png-inline tex\" alt=\"d\" src=\"//upload.wikimedia.org/math/8/2/7/8277e0910d750195b448797616e091ad.png\" />. Blei argues that this step is cheating because you are essentially refitting the model to the new data.</p>"
"<p>Variations on LDA have been used to automatically put natural images into categories, such as \"bedroom\" or \"forest\", by treating an image as a document, and small patches of the image as words;<sup id=\"cite_ref-8\" class=\"reference\"><a href=\"#cite_note-8\"><span>[</span>8<span>]</span></a></sup> one of the variations is called <a href=\"/w/index.php?title=Spatial_Latent_Dirichlet_Allocation&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Spatial Latent Dirichlet Allocation (page does not exist)\">Spatial Latent Dirichlet Allocation</a>.<sup id=\"cite_ref-9\" class=\"reference\"><a href=\"#cite_note-9\"><span>[</span>9<span>]</span></a></sup></p>"
"<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Latent_Dirichlet_allocation&amp;action=edit&amp;section=6\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>"
"<ul>"
"<li><a href=\"/wiki/Pachinko_allocation\" title=\"Pachinko allocation\">Pachinko allocation</a></li>"
"<li><a href=\"/wiki/Tf-idf\" title=\"Tf-idf\" class=\"mw-redirect\">tf-idf</a></li>"
"</ul>"
"<h2><span class=\"mw-headline\" id=\"Notes\">Notes</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Latent_Dirichlet_allocation&amp;action=edit&amp;section=7\" title=\"Edit section: Notes\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>"
"<div class=\"reflist\" style=\"list-style-type: decimal;\">"
"<ol class=\"references\">"
"<li id=\"cite_note-blei2003-1\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-blei2003_1-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-blei2003_1-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><span class=\"citation journal\">Blei, David M.; Ng, Andrew Y.; <a href=\"/wiki/Michael_I._Jordan\" title=\"Michael I. Jordan\">Jordan, Michael I</a> (January 2003). <a rel=\"nofollow\" class=\"external text\" href=\"http://jmlr.csail.mit.edu/papers/v3/blei03a.html\">\"Latent Dirichlet allocation\"</a>. In Lafferty, John. <i><a href=\"/wiki/Journal_of_Machine_Learning_Research\" title=\"Journal of Machine Learning Research\">Journal of Machine Learning Research</a></i> <b>3</b> (4–5): <i>pp.</i> 993–1022. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"http://dx.doi.org/10.1162%2Fjmlr.2003.3.4-5.993\">10.1162/jmlr.2003.3.4-5.993</a>.</span><span title=\"ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALatent+Dirichlet+allocation&amp;rft.atitle=Latent+Dirichlet+allocation&amp;rft.au=Blei%2C+David+M.&amp;rft.aufirst=David+M.&amp;rft.au=Jordan%2C+Michael+I&amp;rft.aulast=Blei&amp;rft.au=Ng%2C+Andrew+Y.&amp;rft.date=January+2003&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fjmlr.csail.mit.edu%2Fpapers%2Fv3%2Fblei03a.html&amp;rft_id=info%3Adoi%2F10.1162%2Fjmlr.2003.3.4-5.993&amp;rft.issue=4%E2%80%935&amp;rft.jtitle=Journal+of+Machine+Learning+Research&amp;rft.pages=%27%27pp.%27%27+993-1022&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=3\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span></li>"
"<li id=\"cite_note-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-2\">^</a></b></span> <span class=\"reference-text\"><span class=\"citation conference\">Girolami, Mark; Kaban, A. (2003). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.cs.bham.ac.uk/~axk/sigir2003_mgak.pdf\">\"On an Equivalence between PLSI and LDA\"</a>. Proceedings of SIGIR 2003. New York: Association for Computing Machinery. <a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/1-58113-646-3\" title=\"Special:BookSources/1-58113-646-3\">1-58113-646-3</a>.</span><span title=\"ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALatent+Dirichlet+allocation&amp;rft.aufirst=Mark&amp;rft.au=Girolami%2C+Mark&amp;rft.au=Kaban%2C+A.&amp;rft.aulast=Girolami&amp;rft.btitle=On+an+Equivalence+between+PLSI+and+LDA&amp;rft.date=2003&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fwww.cs.bham.ac.uk%2F~axk%2Fsigir2003_mgak.pdf&amp;rft.isbn=1-58113-646-3&amp;rft.place=New+York&amp;rft.pub=Association+for+Computing+Machinery&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span></li>"
"<li id=\"cite_note-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-3\">^</a></b></span> <span class=\"reference-text\"><span class=\"citation journal\">Griffiths, Thomas L.; Steyvers, Mark (April 6, 2004). <a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pmc/articles/PMC387300\">\"Finding scientific topics\"</a>. <i>Proceedings of the National Academy of Sciences</i> <b>101</b> (Suppl. 1): 5228–5235. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"http://dx.doi.org/10.1073%2Fpnas.0307752101\">10.1073/pnas.0307752101</a>. <a href=\"/wiki/PubMed_Central\" title=\"PubMed Central\">PMC</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pmc/articles/PMC387300\">387300</a>. <a href=\"/wiki/PubMed_Identifier\" title=\"PubMed Identifier\" class=\"mw-redirect\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/14872004\">14872004</a>.</span><span title=\"ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALatent+Dirichlet+allocation&amp;rft.atitle=Finding+scientific+topics&amp;rft.aufirst=Thomas+L.&amp;rft.au=Griffiths%2C+Thomas+L.&amp;rft.aulast=Griffiths&amp;rft.au=Steyvers%2C+Mark&amp;rft.date=April+6%2C+2004&amp;rft.genre=article&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC387300&amp;rft_id=info%3Adoi%2F10.1073%2Fpnas.0307752101&amp;rft_id=info%3Apmc%2F387300&amp;rft_id=info%3Apmid%2F14872004&amp;rft.issue=Suppl.+1&amp;rft.jtitle=Proceedings+of+the+National+Academy+of+Sciences&amp;rft.pages=5228-5235&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=101\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span></li>"
"<li id=\"cite_note-4\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-4\">^</a></b></span> <span class=\"reference-text\"><span class=\"citation conference\">Minka, Thomas; Lafferty, John (2002). <a rel=\"nofollow\" class=\"external text\" href=\"https://research.microsoft.com/~minka/papers/aspect/minka-aspect.pdf\">\"Expectation-propagation for the generative aspect model\"</a>. Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence. San Francisco, CA: Morgan Kaufmann. <a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/1-55860-897-4\" title=\"Special:BookSources/1-55860-897-4\">1-55860-897-4</a>.</span><span title=\"ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALatent+Dirichlet+allocation&amp;rft.aufirst=Thomas&amp;rft.au=Lafferty%2C+John&amp;rft.aulast=Minka&amp;rft.au=Minka%2C+Thomas&amp;rft.btitle=Expectation-propagation+for+the+generative+aspect+model&amp;rft.date=2002&amp;rft.genre=book&amp;rft_id=https%3A%2F%2Fresearch.microsoft.com%2F~minka%2Fpapers%2Faspect%2Fminka-aspect.pdf&amp;rft.isbn=1-55860-897-4&amp;rft.place=San+Francisco%2C+CA&amp;rft.pub=Morgan+Kaufmann&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span></li>"
"<li id=\"cite_note-5\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-5\">^</a></b></span> <span class=\"reference-text\"><span class=\"citation journal\">Blei, David M.; Lafferty, John D. (2006). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.cs.cmu.edu/~lafferty/pub/ctm.pdf\">\"Correlated topic models\"</a>. <i>Advances in Neural Information Processing Systems</i> <b>18</b>.</span><span title=\"ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALatent+Dirichlet+allocation&amp;rft.atitle=Correlated+topic+models&amp;rft.au=Blei%2C+David+M.&amp;rft.aufirst=David+M.&amp;rft.au=Lafferty%2C+John+D.&amp;rft.aulast=Blei&amp;rft.date=2006&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fwww.cs.cmu.edu%2F~lafferty%2Fpub%2Fctm.pdf&amp;rft.jtitle=Advances+in+Neural+Information+Processing+Systems&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=18\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span></li>"
"<li id=\"cite_note-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-6\">^</a></b></span> <span class=\"reference-text\"><span class=\"citation conference\">Blei, David M.; <a href=\"/wiki/Michael_I._Jordan\" title=\"Michael I. Jordan\">Jordan, Michael I.</a>; Griffiths, Thomas L.; Tenenbaum, Joshua B (2004). <a rel=\"nofollow\" class=\"external text\" href=\"http://cocosci.berkeley.edu/tom/papers/ncrp.pdf\">\"Hierarchical Topic Models and the Nested Chinese Restaurant Process\"</a>. Advances in Neural Information Processing Systems 16: Proceedings of the 2003 Conference. MIT Press. <a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/0-262-20152-6\" title=\"Special:BookSources/0-262-20152-6\">0-262-20152-6</a>.</span><span title=\"ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALatent+Dirichlet+allocation&amp;rft.au=Blei%2C+David+M.&amp;rft.aufirst=David+M.&amp;rft.aulast=Blei&amp;rft.btitle=Hierarchical+Topic+Models+and+the+Nested+Chinese+Restaurant+Process&amp;rft.date=2004&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fcocosci.berkeley.edu%2Ftom%2Fpapers%2Fncrp.pdf&amp;rft.isbn=0-262-20152-6&amp;rft.pub=MIT+Press&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span> <span style=\"display:none;font-size:100%\" class=\"error citation-comment\">Cite uses deprecated parameters (<a href=\"/wiki/Help:CS1_errors#deprecated_params\" title=\"Help:CS1 errors\">help</a>)</span></span></li>"
"<li id=\"cite_note-7\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-7\">^</a></b></span> <span class=\"reference-text\"><span class=\"citation conference\">Shu, Liangcai; Long, Bo; Meng, Weiyi (2009). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.cs.binghamton.edu/~meng/pub.d/icde09-LatentTopic.pdf\">\"A Latent Topic Model for Complete Entity Resolution\"</a>. 25th IEEE International Conference on Data Engineering (ICDE 2009).</span><span title=\"ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALatent+Dirichlet+allocation&amp;rft.aufirst=Liangcai&amp;rft.aulast=Shu&amp;rft.au=Shu%2C+Liangcai&amp;rft.btitle=A+Latent+Topic+Model+for+Complete+Entity+Resolution&amp;rft.date=2009&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fwww.cs.binghamton.edu%2F~meng%2Fpub.d%2Ficde09-LatentTopic.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span> <span style=\"display:none;font-size:100%\" class=\"error citation-comment\">Cite uses deprecated parameters (<a href=\"/wiki/Help:CS1_errors#deprecated_params\" title=\"Help:CS1 errors\">help</a>)</span></span></li>"
"<li id=\"cite_note-8\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-8\">^</a></b></span> <span class=\"reference-text\"><span class=\"citation journal\">Li, Fei-Fei; Perona, Pietro. \"A Bayesian Hierarchical Model for Learning Natural Scene Categories\". <i>Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)</i> <b>2</b>: 524–531.</span><span title=\"ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALatent+Dirichlet+allocation&amp;rft.atitle=A+Bayesian+Hierarchical+Model+for+Learning+Natural+Scene+Categories&amp;rft.aufirst=Fei-Fei&amp;rft.aulast=Li&amp;rft.au=Li%2C+Fei-Fei&amp;rft.au=Perona%2C+Pietro&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+2005+IEEE+Computer+Society+Conference+on+Computer+Vision+and+Pattern+Recognition+%28CVPR%2705%29&amp;rft.pages=524-531&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=2\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span></li>"
"<li id=\"cite_note-9\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-9\">^</a></b></span> <span class=\"reference-text\"><span class=\"citation journal\">Wang, Xiaogang; Grimson, Eric (2007). <a rel=\"nofollow\" class=\"external text\" href=\"http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2007_102.pdf\">\"Spatial Latent Dirichlet Allocation\"</a>. <i>Proceedings of Neural Information Processing Systems Conference (NIPS)</i>.</span><span title=\"ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALatent+Dirichlet+allocation&amp;rft.atitle=Spatial+Latent+Dirichlet+Allocation&amp;rft.aufirst=Xiaogang&amp;rft.au=Grimson%2C+Eric&amp;rft.aulast=Wang&amp;rft.au=Wang%2C+Xiaogang&amp;rft.date=2007&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fmachinelearning.wustl.edu%2Fmlpapers%2Fpaper_files%2FNIPS2007_102.pdf&amp;rft.jtitle=Proceedings+of+Neural+Information+Processing+Systems+Conference+%28NIPS%29&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span></li>"
"</ol>"
"</div>"
"<h2><span class=\"mw-headline\" id=\"External_links\">External links</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Latent_Dirichlet_allocation&amp;action=edit&amp;section=8\" title=\"Edit section: External links\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>"
"<ul>"
"<li>Extremely useful lecture for understanding LDA <a rel=\"nofollow\" class=\"external text\" href=\"http://videolectures.net/mlss09uk_blei_tm/\">LDA and Topic Modelling Video Lecture by David Blei</a></li>"
"<li><a rel=\"nofollow\" class=\"external text\" href=\"http://mimno.infosci.cornell.edu/topics.html\">D. Mimno's LDA Bibliography</a> An exhaustive list of LDA-related resources (incl. papers and some implementations)</li>"
"<li><a href=\"/wiki/Gensim\" title=\"Gensim\">Gensim</a>, a fast Python+<a href=\"/wiki/NumPy\" title=\"NumPy\">NumPy</a> implementation of online LDA for inputs larger than the available RAM.</li>"
"<li><a rel=\"nofollow\" class=\"external text\" href=\"http://cran.r-project.org/web/packages/topicmodels/index.html\">topicmodels</a> and <a rel=\"nofollow\" class=\"external text\" href=\"http://cran.r-project.org/web/packages/lda/index.html\">lda</a> are two <a href=\"/wiki/R_(programming_language)\" title=\"R (programming language)\">R</a> packages for LDA analysis.</li>"
"<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.r-bloggers.com/RUG/2010/10/285/\">\"Text Mining with R\" including LDA methods</a>, video presentation to the October 2011 meeting of the Los Angeles R users group</li>"
"<li><a rel=\"nofollow\" class=\"external text\" href=\"http://mallet.cs.umass.edu/index.php\">MALLET</a> Open source Java-based package from the University of Massachusetts-Amherst for topic modeling with LDA, also has an independently developed GUI, the <a rel=\"nofollow\" class=\"external text\" href=\"http://code.google.com/p/topic-modeling-tool/\">Topic Modeling Tool</a></li>"
"<li><a rel=\"nofollow\" class=\"external text\" href=\"https://cwiki.apache.org/confluence/display/MAHOUT/Latent+Dirichlet+Allocation\">LDA in Mahout</a> implementation of LDA using <a href=\"/wiki/MapReduce\" title=\"MapReduce\">MapReduce</a> on the <a href=\"/wiki/Hadoop\" title=\"Hadoop\" class=\"mw-redirect\">Hadoop</a> platform</li>"
"<li><a rel=\"nofollow\" class=\"external text\" href=\"http://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/Latent%20Dirichlet%20Allocation.aspx\">Latent Dirichlet Allocation (LDA) Tutorial for the Infer.NET Machine Computing Framework</a> Microsoft Research C# Machine Learning Framework</li>"
"</ul>"
"<!-- "
"NewPP limit report"
"Parsed by mw1092"
"CPU time usage: 0.796 seconds"
"Real time usage: 4.466 seconds"
"Preprocessor visited node count: 1650/1000000"
"Preprocessor generated node count: 5529/1500000"
"Post‐expand include size: 19817/2048000 bytes"
"Template argument size: 574/2048000 bytes"
"Highest expansion depth: 11/40"
"Expensive parser function count: 1/500"
"Lua time usage: 0.059/10.000 seconds"
"Lua memory usage: 2.27 MB/50 MB"
"-->"
"<!-- Saved in parser cache with key enwiki:pcache:idhash:4605351-0!*!0!!en!4!*!math=0 and timestamp 20140606184958 and revision id 610319663"
" -->"
"<noscript><img src=\"//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1\" alt=\"\" title=\"\" width=\"1\" height=\"1\" style=\"border: none; position: absolute;\" /></noscript></div>									<div class=\"printfooter\">"
"						Retrieved from \"<a dir=\"ltr\" href=\"http://en.wikipedia.org/w/index.php?title=Latent_Dirichlet_allocation&amp;oldid=610319663\">http://en.wikipedia.org/w/index.php?title=Latent_Dirichlet_allocation&amp;oldid=610319663</a>\"					</div>"
"													<div id='catlinks' class='catlinks'><div id=\"mw-normal-catlinks\" class=\"mw-normal-catlinks\"><a href=\"/wiki/Help:Category\" title=\"Help:Category\">Categories</a>: <ul><li><a href=\"/wiki/Category:Statistical_natural_language_processing\" title=\"Category:Statistical natural language processing\">Statistical natural language processing</a></li><li><a href=\"/wiki/Category:Latent_variable_models\" title=\"Category:Latent variable models\">Latent variable models</a></li><li><a href=\"/wiki/Category:Probabilistic_models\" title=\"Category:Probabilistic models\">Probabilistic models</a></li></ul></div><div id=\"mw-hidden-catlinks\" class=\"mw-hidden-catlinks mw-hidden-cats-hidden\">Hidden categories: <ul><li><a href=\"/wiki/Category:Pages_containing_cite_templates_with_deprecated_parameters\" title=\"Category:Pages containing cite templates with deprecated parameters\">Pages containing cite templates with deprecated parameters</a></li><li><a href=\"/wiki/Category:All_articles_with_unsourced_statements\" title=\"Category:All articles with unsourced statements\">All articles with unsourced statements</a></li><li><a href=\"/wiki/Category:Articles_with_unsourced_statements_from_September_2013\" title=\"Category:Articles with unsourced statements from September 2013\">Articles with unsourced statements from September 2013</a></li></ul></div></div>												<div class=\"visualClear\"></div>"
"							</div>"
"		</div>"
"		<div id=\"mw-navigation\">"
"			<h2>Navigation menu</h2>"
"			<div id=\"mw-head\">"
"									<div id=\"p-personal\" role=\"navigation\" class=\"\" aria-labelledby=\"p-personal-label\">"
"						<h3 id=\"p-personal-label\">Personal tools</h3>"
"						<ul>"
"							<li id=\"pt-createaccount\"><a href=\"/w/index.php?title=Special:UserLogin&amp;returnto=Latent+Dirichlet+allocation&amp;type=signup\">Create account</a></li><li id=\"pt-login\"><a href=\"/w/index.php?title=Special:UserLogin&amp;returnto=Latent+Dirichlet+allocation\" title=\"You're encouraged to log in; however, it's not mandatory. [o]\" accesskey=\"o\">Log in</a></li>						</ul>"
"					</div>"
"									<div id=\"left-navigation\">"
"										<div id=\"p-namespaces\" role=\"navigation\" class=\"vectorTabs\" aria-labelledby=\"p-namespaces-label\">"
"						<h3 id=\"p-namespaces-label\">Namespaces</h3>"
"						<ul>"
"															<li  id=\"ca-nstab-main\" class=\"selected\"><span><a href=\"/wiki/Latent_Dirichlet_allocation\"  title=\"View the content page [c]\" accesskey=\"c\">Article</a></span></li>"
"															<li  id=\"ca-talk\"><span><a href=\"/wiki/Talk:Latent_Dirichlet_allocation\"  title=\"Discussion about the content page [t]\" accesskey=\"t\">Talk</a></span></li>"
"													</ul>"
"					</div>"
"										<div id=\"p-variants\" role=\"navigation\" class=\"vectorMenu emptyPortlet\" aria-labelledby=\"p-variants-label\">"
"						<h3 id=\"mw-vector-current-variant\">"
"													</h3>"
"						<h3 id=\"p-variants-label\"><span>Variants</span><a href=\"#\"></a></h3>"
"						<div class=\"menu\">"
"							<ul>"
"															</ul>"
"						</div>"
"					</div>"
"									</div>"
"				<div id=\"right-navigation\">"
"										<div id=\"p-views\" role=\"navigation\" class=\"vectorTabs\" aria-labelledby=\"p-views-label\">"
"						<h3 id=\"p-views-label\">Views</h3>"
"						<ul>"
"															<li id=\"ca-view\" class=\"selected\"><span><a href=\"/wiki/Latent_Dirichlet_allocation\" >Read</a></span></li>"
"															<li id=\"ca-edit\"><span><a href=\"/w/index.php?title=Latent_Dirichlet_allocation&amp;action=edit\"  title=\"You can edit this page. Please use the preview button before saving [e]\" accesskey=\"e\">Edit</a></span></li>"
"															<li id=\"ca-history\" class=\"collapsible\"><span><a href=\"/w/index.php?title=Latent_Dirichlet_allocation&amp;action=history\"  title=\"Past versions of this page [h]\" accesskey=\"h\">View history</a></span></li>"
"													</ul>"
"					</div>"
"										<div id=\"p-cactions\" role=\"navigation\" class=\"vectorMenu emptyPortlet\" aria-labelledby=\"p-cactions-label\">"
"						<h3 id=\"p-cactions-label\"><span>More</span><a href=\"#\"></a></h3>"
"						<div class=\"menu\">"
"							<ul>"
"															</ul>"
"						</div>"
"					</div>"
"										<div id=\"p-search\" role=\"search\">"
"						<h3>"
"							<label for=\"searchInput\">Search</label>"
"						</h3>"
"						<form action=\"/w/index.php\" id=\"searchform\">"
"														<div id=\"simpleSearch\">"
"															<input type=\"search\" name=\"search\" placeholder=\"Search\" title=\"Search Wikipedia [f]\" accesskey=\"f\" id=\"searchInput\" /><input type=\"hidden\" value=\"Special:Search\" name=\"title\" /><input type=\"submit\" name=\"fulltext\" value=\"Search\" title=\"Search Wikipedia for this text\" id=\"mw-searchButton\" class=\"searchButton mw-fallbackSearchButton\" /><input type=\"submit\" name=\"go\" value=\"Go\" title=\"Go to a page with this exact name if one exists\" id=\"searchButton\" class=\"searchButton\" />								</div>"
"						</form>"
"					</div>"
"									</div>"
"			</div>"
"			<div id=\"mw-panel\">"
"				<div id=\"p-logo\" role=\"banner\"><a style=\"background-image: url(//upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);\" href=\"/wiki/Main_Page\"  title=\"Visit the main page\"></a></div>"
"						<div class=\"portal\" role=\"navigation\" id='p-navigation' aria-labelledby='p-navigation-label'>"
"			<h3 id='p-navigation-label'>Navigation</h3>"
"			<div class=\"body\">"
"									<ul>"
"													<li id=\"n-mainpage-description\"><a href=\"/wiki/Main_Page\" title=\"Visit the main page [z]\" accesskey=\"z\">Main page</a></li>"
"													<li id=\"n-contents\"><a href=\"/wiki/Portal:Contents\" title=\"Guides to browsing Wikipedia\">Contents</a></li>"
"													<li id=\"n-featuredcontent\"><a href=\"/wiki/Portal:Featured_content\" title=\"Featured content – the best of Wikipedia\">Featured content</a></li>"
"													<li id=\"n-currentevents\"><a href=\"/wiki/Portal:Current_events\" title=\"Find background information on current events\">Current events</a></li>"
"													<li id=\"n-randompage\"><a href=\"/wiki/Special:Random\" title=\"Load a random article [x]\" accesskey=\"x\">Random article</a></li>"
"													<li id=\"n-sitesupport\"><a href=\"https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en\" title=\"Support us\">Donate to Wikipedia</a></li>"
"													<li id=\"n-shoplink\"><a href=\"//shop.wikimedia.org\" title=\"Visit the Wikimedia Shop\">Wikimedia Shop</a></li>"
"											</ul>"
"							</div>"
"		</div>"
"			<div class=\"portal\" role=\"navigation\" id='p-interaction' aria-labelledby='p-interaction-label'>"
"			<h3 id='p-interaction-label'>Interaction</h3>"
"			<div class=\"body\">"
"									<ul>"
"													<li id=\"n-help\"><a href=\"/wiki/Help:Contents\" title=\"Guidance on how to use and edit Wikipedia\">Help</a></li>"
"													<li id=\"n-aboutsite\"><a href=\"/wiki/Wikipedia:About\" title=\"Find out about Wikipedia\">About Wikipedia</a></li>"
"													<li id=\"n-portal\"><a href=\"/wiki/Wikipedia:Community_portal\" title=\"About the project, what you can do, where to find things\">Community portal</a></li>"
"													<li id=\"n-recentchanges\"><a href=\"/wiki/Special:RecentChanges\" title=\"A list of recent changes in the wiki [r]\" accesskey=\"r\">Recent changes</a></li>"
"													<li id=\"n-contactpage\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\">Contact page</a></li>"
"											</ul>"
"							</div>"
"		</div>"
"			<div class=\"portal\" role=\"navigation\" id='p-tb' aria-labelledby='p-tb-label'>"
"			<h3 id='p-tb-label'>Tools</h3>"
"			<div class=\"body\">"
"									<ul>"
"													<li id=\"t-whatlinkshere\"><a href=\"/wiki/Special:WhatLinksHere/Latent_Dirichlet_allocation\" title=\"List of all English Wikipedia pages containing links to this page [j]\" accesskey=\"j\">What links here</a></li>"
"													<li id=\"t-recentchangeslinked\"><a href=\"/wiki/Special:RecentChangesLinked/Latent_Dirichlet_allocation\" title=\"Recent changes in pages linked from this page [k]\" accesskey=\"k\">Related changes</a></li>"
"													<li id=\"t-upload\"><a href=\"/wiki/Wikipedia:File_Upload_Wizard\" title=\"Upload files [u]\" accesskey=\"u\">Upload file</a></li>"
"													<li id=\"t-specialpages\"><a href=\"/wiki/Special:SpecialPages\" title=\"A list of all special pages [q]\" accesskey=\"q\">Special pages</a></li>"
"													<li id=\"t-permalink\"><a href=\"/w/index.php?title=Latent_Dirichlet_allocation&amp;oldid=610319663\" title=\"Permanent link to this revision of the page\">Permanent link</a></li>"
"													<li id=\"t-info\"><a href=\"/w/index.php?title=Latent_Dirichlet_allocation&amp;action=info\">Page information</a></li>"
"													<li id=\"t-wikibase\"><a href=\"//www.wikidata.org/wiki/Q269236\" title=\"Link to connected data repository item [g]\" accesskey=\"g\">Data item</a></li>"
"						<li id=\"t-cite\"><a href=\"/w/index.php?title=Special:Cite&amp;page=Latent_Dirichlet_allocation&amp;id=610319663\" title=\"Information on how to cite this page\">Cite this page</a></li>					</ul>"
"							</div>"
"		</div>"
"			<div class=\"portal\" role=\"navigation\" id='p-coll-print_export' aria-labelledby='p-coll-print_export-label'>"
"			<h3 id='p-coll-print_export-label'>Print/export</h3>"
"			<div class=\"body\">"
"									<ul>"
"													<li id=\"coll-create_a_book\"><a href=\"/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Latent+Dirichlet+allocation\">Create a book</a></li>"
"													<li id=\"coll-download-as-rl\"><a href=\"/w/index.php?title=Special:Book&amp;bookcmd=render_article&amp;arttitle=Latent+Dirichlet+allocation&amp;oldid=610319663&amp;writer=rl\">Download as PDF</a></li>"
"													<li id=\"t-print\"><a href=\"/w/index.php?title=Latent_Dirichlet_allocation&amp;printable=yes\" title=\"Printable version of this page [p]\" accesskey=\"p\">Printable version</a></li>"
"											</ul>"
"							</div>"
"		</div>"
"			<div class=\"portal\" role=\"navigation\" id='p-lang' aria-labelledby='p-lang-label'>"
"			<h3 id='p-lang-label'>Languages</h3>"
"			<div class=\"body\">"
"									<ul>"
"													<li class=\"interlanguage-link interwiki-de\"><a href=\"//de.wikipedia.org/wiki/Latent_Dirichlet_Allocation\" title=\"Latent Dirichlet Allocation – German\" lang=\"de\" hreflang=\"de\">Deutsch</a></li>"
"													<li class=\"interlanguage-link interwiki-es\"><a href=\"//es.wikipedia.org/wiki/Latent_Dirichlet_Allocation\" title=\"Latent Dirichlet Allocation – Spanish\" lang=\"es\" hreflang=\"es\">Español</a></li>"
"													<li class=\"interlanguage-link interwiki-fa\"><a href=\"//fa.wikipedia.org/wiki/%D8%AA%D8%AE%D8%B5%DB%8C%D8%B5_%D9%BE%D9%86%D9%87%D8%A7%D9%86_%D8%AF%DB%8C%D8%B1%DB%8C%DA%A9%D9%84%D9%87\" title=\"تخصیص پنهان دیریکله – Persian\" lang=\"fa\" hreflang=\"fa\">فارسی</a></li>"
"													<li class=\"interlanguage-link interwiki-fr\"><a href=\"//fr.wikipedia.org/wiki/Allocation_de_Dirichlet_latente\" title=\"Allocation de Dirichlet latente – French\" lang=\"fr\" hreflang=\"fr\">Français</a></li>"
"													<li class=\"interlanguage-link interwiki-ko\"><a href=\"//ko.wikipedia.org/wiki/%EC%9E%A0%EC%9E%AC_%EB%94%94%EB%A6%AC%ED%81%B4%EB%A0%88_%ED%95%A0%EB%8B%B9\" title=\"잠재 디리클레 할당 – Korean\" lang=\"ko\" hreflang=\"ko\">한국어</a></li>"
"													<li class=\"interlanguage-link interwiki-ru\"><a href=\"//ru.wikipedia.org/wiki/%D0%9B%D0%B0%D1%82%D0%B5%D0%BD%D1%82%D0%BD%D0%BE%D0%B5_%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%89%D0%B5%D0%BD%D0%B8%D0%B5_%D0%94%D0%B8%D1%80%D0%B8%D1%85%D0%BB%D0%B5\" title=\"Латентное размещение Дирихле – Russian\" lang=\"ru\" hreflang=\"ru\">Русский</a></li>"
"													<li class=\"interlanguage-link interwiki-zh\"><a href=\"//zh.wikipedia.org/wiki/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\" title=\"隐含狄利克雷分布 – Chinese\" lang=\"zh\" hreflang=\"zh\">中文</a></li>"
"													<li class=\"uls-p-lang-dummy\"><a href=\"#\"></a></li>"
"											</ul>"
"				<div class='after-portlet after-portlet-lang'><span class=\"wb-langlinks-edit wb-langlinks-link\"><a action=\"edit\" href=\"//www.wikidata.org/wiki/Q269236#sitelinks-wikipedia\" text=\"Edit links\" title=\"Edit interlanguage links\" class=\"wbc-editpage\">Edit links</a></span></div>			</div>"
"		</div>"
"				</div>"
"		</div>"
"		<div id=\"footer\" role=\"contentinfo\">"
"							<ul id=\"footer-info\">"
"											<li id=\"footer-info-lastmod\"> This page was last modified on 27 May 2014 at 08:37.<br /></li>"
"											<li id=\"footer-info-copyright\">Text is available under the <a rel=\"license\" href=\"//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\">Creative Commons Attribution-ShareAlike License</a><a rel=\"license\" href=\"//creativecommons.org/licenses/by-sa/3.0/\" style=\"display:none;\"></a>;"
"additional terms may apply.  By using this site, you agree to the <a href=\"//wikimediafoundation.org/wiki/Terms_of_Use\">Terms of Use</a> and <a href=\"//wikimediafoundation.org/wiki/Privacy_policy\">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href=\"//www.wikimediafoundation.org/\">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>"
"									</ul>"
"							<ul id=\"footer-places\">"
"											<li id=\"footer-places-privacy\"><a href=\"//wikimediafoundation.org/wiki/Privacy_policy\" title=\"wikimedia:Privacy policy\">Privacy policy</a></li>"
"											<li id=\"footer-places-about\"><a href=\"/wiki/Wikipedia:About\" title=\"Wikipedia:About\">About Wikipedia</a></li>"
"											<li id=\"footer-places-disclaimer\"><a href=\"/wiki/Wikipedia:General_disclaimer\" title=\"Wikipedia:General disclaimer\">Disclaimers</a></li>"
"											<li id=\"footer-places-contact\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\">Contact Wikipedia</a></li>"
"											<li id=\"footer-places-developers\"><a href=\"https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute\">Developers</a></li>"
"											<li id=\"footer-places-mobileview\"><a href=\"//en.m.wikipedia.org/wiki/Latent_Dirichlet_allocation\" class=\"noprint stopMobileRedirectToggle\">Mobile view</a></li>"
"									</ul>"
"										<ul id=\"footer-icons\" class=\"noprint\">"
"											<li id=\"footer-copyrightico\">"
"															<a href=\"//wikimediafoundation.org/\"><img src=\"//bits.wikimedia.org/images/wikimedia-button.png\" width=\"88\" height=\"31\" alt=\"Wikimedia Foundation\"/></a>"
"													</li>"
"											<li id=\"footer-poweredbyico\">"
"															<a href=\"//www.mediawiki.org/\"><img src=\"//bits.wikimedia.org/static-1.24wmf10/skins/common/images/poweredby_mediawiki_88x31.png\" alt=\"Powered by MediaWiki\" width=\"88\" height=\"31\" /></a>"
"													</li>"
"									</ul>"
"						<div style=\"clear:both\"></div>"
"		</div>"
"		<script>/*<![CDATA[*/window.jQuery && jQuery.ready();/*]]>*/</script><script>if(window.mw){"
"mw.loader.state({\"site\":\"loading\",\"user\":\"ready\",\"user.groups\":\"ready\"});"
"}</script>"
"<script>if(window.mw){"
"mw.loader.load([\"ext.cite\",\"ext.math.mathjax.enabler\",\"mediawiki.toc\",\"mobile.desktop\",\"mediawiki.action.view.postEdit\",\"mediawiki.user\",\"mediawiki.hidpi\",\"mediawiki.page.ready\",\"mediawiki.searchSuggest\",\"ext.gadget.teahouse\",\"ext.gadget.ReferenceTooltips\",\"ext.gadget.DRN-wizard\",\"ext.gadget.charinsert\",\"ext.gadget.refToolbar\",\"mw.MwEmbedSupport.style\",\"mmv.bootstrap.autostart\",\"ext.eventLogging.subscriber\",\"ext.navigationTiming\",\"schema.UniversalLanguageSelector\",\"ext.uls.eventlogger\",\"ext.uls.interlanguage\"],null,true);"
"}</script>"
"<script src=\"//bits.wikimedia.org/en.wikipedia.org/load.php?debug=false&amp;lang=en&amp;modules=site&amp;only=scripts&amp;skin=vector&amp;*\"></script>"
"<script>if(window.mw){"
"mw.config.set({\"wgBackendResponseTime\":296,\"wgHostname\":\"mw1079\"});"
"}</script>"
"	</body>"
"</html>"
"	"
